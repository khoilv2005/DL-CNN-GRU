# BÃO CÃO TOÃ€N DIá»†N: MÃ” HÃŒNH CNN-GRU CHO PHÃT HIá»†N XÃ‚M NHáº¬P IoT

## ğŸ“‹ Má»¤C Lá»¤C

1. [Tá»•ng Quan Há»‡ Thá»‘ng](#1-tá»•ng-quan-há»‡-thá»‘ng)
2. [Kiáº¿n TrÃºc MÃ´ HÃ¬nh](#2-kiáº¿n-trÃºc-mÃ´-hÃ¬nh)
3. [Chi Tiáº¿t CÃ¡c Module](#3-chi-tiáº¿t-cÃ¡c-module)
4. [CÃ´ng Thá»©c ToÃ¡n Há»c](#4-cÃ´ng-thá»©c-toÃ¡n-há»c)
5. [QuÃ¡ TrÃ¬nh Tiá»n Xá»­ LÃ½ Dá»¯ Liá»‡u](#5-quÃ¡-trÃ¬nh-tiá»n-xá»­-lÃ½-dá»¯-liá»‡u)
6. [Thuáº­t ToÃ¡n Huáº¥n Luyá»‡n](#6-thuáº­t-toÃ¡n-huáº¥n-luyá»‡n)
7. [Xá»­ LÃ½ Imbalanced Data](#7-xá»­-lÃ½-imbalanced-data)
8. [Callbacks vÃ  Optimization](#8-callbacks-vÃ -optimization)
9. [ÄÃ¡nh GiÃ¡ MÃ´ HÃ¬nh](#9-Ä‘Ã¡nh-giÃ¡-mÃ´-hÃ¬nh)
10. [Káº¿t Luáº­n](#10-káº¿t-luáº­n)

---

## 1. Tá»”NG QUAN Há»† THá»NG

### 1.1. Má»¥c ÄÃ­ch
XÃ¢y dá»±ng há»‡ thá»‘ng phÃ¡t hiá»‡n xÃ¢m nháº­p (Intrusion Detection System - IDS) cho mÃ´i trÆ°á»ng IoT sá»­ dá»¥ng Deep Learning, dá»±a trÃªn kiáº¿n trÃºc **DeepFed**.

### 1.2. BÃ i ToÃ¡n
- **Loáº¡i bÃ i toÃ¡n**: Binary Classification (PhÃ¢n loáº¡i nhá»‹ phÃ¢n)
- **Input**: Network traffic features (39 features)
- **Output**: 2 classes
  - Class 0: **Attack** (táº¥n cÃ´ng)
  - Class 1: **Benign** (lÆ°u lÆ°á»£ng bÃ¬nh thÆ°á»ng)

### 1.3. Dataset
- **TÃªn**: IoT Dataset 2023
- **Tá»•ng sá»‘ máº«u**: ~15 triá»‡u samples
- **Sá»‘ features**: 39 features (sau khi loáº¡i bá» constant columns)
- **PhÃ¢n bá»‘**:
  - Attack: 14,725,951 máº«u (97.66%)
  - Benign: 352,302 máº«u (2.34%)
  - **Imbalance Ratio**: 41.80:1

### 1.4. Chia Dá»¯ Liá»‡u
```
Training Set:   70% (~10.5 triá»‡u máº«u)
Validation Set: 10% (~1.5 triá»‡u máº«u)
Test Set:       20% (~3.0 triá»‡u máº«u)
```

---

## 2. KIáº¾N TRÃšC MÃ” HÃŒNH

### 2.1. SÆ¡ Äá»“ Tá»•ng Quan

```
                        INPUT (39 features)
                              |
                    +---------+---------+
                    |                   |
               [CNN Module]        [GRU Module]
                    |                   |
              Conv1D -> BN          GRU Layer 1
              MaxPool              (128 units)
                    |                   |
              Conv1D -> BN          GRU Layer 2
              MaxPool              (64 units)
                    |                   |
              Conv1D -> BN              |
              MaxPool                   |
                    |                   |
                 Flatten                |
                    |                   |
                    +--------+----------+
                             |
                      [Concatenate]
                             |
                      [MLP Module]
                             |
                    Dense(256) -> BN -> Dropout(0.5)
                             |
                    Dense(128) -> BN -> Dropout(0.3)
                             |
                     Dense(2) + Softmax
                             |
                    OUTPUT (2 classes)
```

### 2.2. ThÃ´ng Sá»‘ MÃ´ HÃ¬nh
- **Tá»•ng sá»‘ parameters**: 526,338 parameters (~2 MB)
- **Trainable parameters**: 524,674
- **Non-trainable parameters**: 1,664 (BatchNormalization)

---

## 3. CHI TIáº¾T CÃC MODULE

### 3.1. CNN Module (Convolutional Neural Network)

**Má»¥c Ä‘Ã­ch**: TrÃ­ch xuáº¥t **spatial features** (Ä‘áº·c trÆ°ng khÃ´ng gian) tá»« dá»¯ liá»‡u.

#### Cáº¥u trÃºc:
```python
Input: (batch_size, 39)
  â†“
Reshape: (batch_size, 39, 1)  # Chuyá»ƒn thÃ nh dáº¡ng time series
  â†“
Conv Block 1:
  - Conv1D(64 filters, kernel=3, activation=ReLU)
  - BatchNormalization
  - MaxPooling1D(pool_size=2)
  Output: (batch_size, 19, 64)
  â†“
Conv Block 2:
  - Conv1D(128 filters, kernel=3, activation=ReLU)
  - BatchNormalization
  - MaxPooling1D(pool_size=2)
  Output: (batch_size, 9, 128)
  â†“
Conv Block 3:
  - Conv1D(256 filters, kernel=3, activation=ReLU)
  - BatchNormalization
  - MaxPooling1D(pool_size=2)
  Output: (batch_size, 4, 256)
  â†“
Flatten: (batch_size, 1024)
```

#### Vai trÃ² tá»«ng thÃ nh pháº§n:

**Conv1D (Convolutional 1D):**
- QuÃ©t qua chuá»—i features vá»›i sliding window
- PhÃ¡t hiá»‡n patterns cá»¥c bá»™ trong dá»¯ liá»‡u
- Má»—i filter há»c má»™t pattern khÃ¡c nhau

**BatchNormalization:**
- Chuáº©n hÃ³a output cá»§a má»—i layer
- TÄƒng tá»‘c Ä‘á»™ training
- Giáº£m Internal Covariate Shift

**MaxPooling:**
- Giáº£m kÃ­ch thÆ°á»›c dá»¯ liá»‡u (downsampling)
- Giá»¯ láº¡i features quan trá»ng nháº¥t
- Táº¡o translation invariance

---

### 3.2. GRU Module (Gated Recurrent Unit)

**Má»¥c Ä‘Ã­ch**: TrÃ­ch xuáº¥t **temporal features** (Ä‘áº·c trÆ°ng thá»i gian) vÃ  phá»¥ thuá»™c tuáº§n tá»±.

#### Cáº¥u trÃºc:
```python
Input: (batch_size, 39)
  â†“
Reshape: (batch_size, 39, 1)
  â†“
GRU Layer 1:
  - 128 units
  - return_sequences=True
  Output: (batch_size, 39, 128)
  â†“
GRU Layer 2:
  - 64 units
  - return_sequences=False
  Output: (batch_size, 64)
```

#### Táº¡i sao chá»n GRU thay vÃ¬ LSTM?
- **GRU nhanh hÆ¡n**: Ãt parameters hÆ¡n LSTM (2 gates vs 3 gates)
- **Hiá»‡u quáº£ tÆ°Æ¡ng Ä‘Æ°Æ¡ng**: Vá»›i dataset lá»›n, GRU cho káº¿t quáº£ gáº§n nhÆ° LSTM
- **TrÃ¡nh overfitting**: Ãt parameters = giáº£m risk overfitting

---

### 3.3. MLP Module (Multi-Layer Perceptron)

**Má»¥c Ä‘Ã­ch**: Káº¿t há»£p features tá»« CNN vÃ  GRU, thá»±c hiá»‡n classification.

#### Cáº¥u trÃºc:
```python
Input: Concatenate([CNN_output, GRU_output])
       Shape: (batch_size, 1024 + 64 = 1088)
  â†“
Dense Layer 1:
  - 256 units, activation=ReLU
  - BatchNormalization
  - Dropout(0.5)
  Output: (batch_size, 256)
  â†“
Dense Layer 2:
  - 128 units, activation=ReLU
  - BatchNormalization
  - Dropout(0.3)
  Output: (batch_size, 128)
  â†“
Output Layer:
  - 2 units, activation=Softmax
  Output: (batch_size, 2)  # [P(Attack), P(Benign)]
```

#### Vai trÃ² Dropout:
- **Dropout(0.5)**: Táº¯t ngáº«u nhiÃªn 50% neurons trong training
- **Dropout(0.3)**: Táº¯t ngáº«u nhiÃªn 30% neurons
- **Má»¥c Ä‘Ã­ch**: TrÃ¡nh overfitting, tÄƒng generalization

---

## 4. CÃ”NG THá»¨C TOÃN Há»ŒC

### 4.1. Convolutional Layer

**CÃ´ng thá»©c Conv1D:**
```
y[i] = Ïƒ(Î£(w[k] * x[i+k]) + b)

Trong Ä‘Ã³:
- x: input sequence
- w: filter weights (kernel)
- b: bias
- Ïƒ: activation function (ReLU)
- k: kernel size
```

**ReLU Activation:**
```
ReLU(x) = max(0, x)
```

**Æ¯u Ä‘iá»ƒm ReLU:**
- TÃ­nh toÃ¡n nhanh
- Giáº£m vanishing gradient
- Táº¡o sparsity (nhiá»u neurons = 0)

---

### 4.2. Batch Normalization

**CÃ´ng thá»©c:**
```
Step 1: TÃ­nh mean vÃ  variance cá»§a mini-batch
Î¼_B = (1/m) * Î£(x_i)
ÏƒÂ²_B = (1/m) * Î£(x_i - Î¼_B)Â²

Step 2: Normalize
xÌ‚_i = (x_i - Î¼_B) / âˆš(ÏƒÂ²_B + Îµ)

Step 3: Scale vÃ  shift
y_i = Î³ * xÌ‚_i + Î²

Trong Ä‘Ã³:
- Î¼_B: mean cá»§a batch
- ÏƒÂ²_B: variance cá»§a batch
- Îµ: sá»‘ nhá» trÃ¡nh chia cho 0 (thÆ°á»ng = 1e-5)
- Î³, Î²: learnable parameters
```

**Lá»£i Ã­ch:**
- á»”n Ä‘á»‹nh quÃ¡ trÃ¬nh training
- Cho phÃ©p learning rate cao hÆ¡n
- Giáº£m phá»¥ thuá»™c vÃ o initialization

---

### 4.3. MaxPooling

**CÃ´ng thá»©c:**
```
y[i] = max(x[i*stride : i*stride + pool_size])

VÃ­ dá»¥ vá»›i pool_size=2:
Input:  [3, 7, 2, 9, 4, 6]
Output: [7, 9, 6]
```

**Æ¯u Ä‘iá»ƒm:**
- Giáº£m computational cost
- Táº¡o invariance to small translations
- Giá»¯ láº¡i features máº¡nh nháº¥t

---

### 4.4. GRU (Gated Recurrent Unit)

**CÃ´ng thá»©c GRU:**

GRU cÃ³ 2 gates: **Reset Gate** vÃ  **Update Gate**

```
1. Reset Gate (r_t):
   r_t = Ïƒ(W_r Â· [h_{t-1}, x_t] + b_r)

2. Update Gate (z_t):
   z_t = Ïƒ(W_z Â· [h_{t-1}, x_t] + b_z)

3. Candidate Hidden State (hÌƒ_t):
   hÌƒ_t = tanh(W_h Â· [r_t âŠ™ h_{t-1}, x_t] + b_h)

4. Final Hidden State (h_t):
   h_t = (1 - z_t) âŠ™ h_{t-1} + z_t âŠ™ hÌƒ_t

Trong Ä‘Ã³:
- x_t: input táº¡i thá»i Ä‘iá»ƒm t
- h_t: hidden state táº¡i thá»i Ä‘iá»ƒm t
- Ïƒ: sigmoid function
- âŠ™: element-wise multiplication
- W, b: learnable weights vÃ  biases
```

**Giáº£i thÃ­ch:**

1. **Reset Gate (r_t)**: Quyáº¿t Ä‘á»‹nh bá» qua bao nhiÃªu thÃ´ng tin tá»« quÃ¡ khá»©
   - r_t â‰ˆ 0: Bá» qua háº¿t thÃ´ng tin cÅ©
   - r_t â‰ˆ 1: Giá»¯ láº¡i toÃ n bá»™ thÃ´ng tin cÅ©

2. **Update Gate (z_t)**: Quyáº¿t Ä‘á»‹nh cáº­p nháº­t bao nhiÃªu thÃ´ng tin má»›i
   - z_t â‰ˆ 0: Giá»¯ nguyÃªn h_{t-1}
   - z_t â‰ˆ 1: Thay tháº¿ hoÃ n toÃ n báº±ng hÌƒ_t

3. **Candidate (hÌƒ_t)**: ThÃ´ng tin má»›i Ä‘Æ°á»£c tÃ­nh toÃ¡n

4. **Final State (h_t)**: Tá»• há»£p giá»¯a thÃ´ng tin cÅ© vÃ  má»›i

---

### 4.5. Dense Layer (Fully Connected)

**CÃ´ng thá»©c:**
```
y = Ïƒ(W Â· x + b)

Trong Ä‘Ã³:
- x: input vector (shape: n)
- W: weight matrix (shape: m Ã— n)
- b: bias vector (shape: m)
- Ïƒ: activation function
- y: output vector (shape: m)
```

**Matrix Multiplication:**
```
y[i] = Î£(W[i,j] * x[j]) + b[i]  for j = 1 to n
```

---

### 4.6. Dropout

**CÃ´ng thá»©c (Training):**
```
y[i] = {
    0              with probability p
    x[i]/(1-p)     with probability (1-p)
}

VÃ­ dá»¥ vá»›i p=0.5 (Dropout 50%):
Input:  [2, 4, 6, 8]
Mask:   [1, 0, 1, 0]  (random)
Output: [4, 0, 12, 0]  (scaled by 1/(1-0.5)=2)
```

**Inference (Testing):**
```
y[i] = x[i]  (khÃ´ng dropout)
```

**Táº¡i sao scale báº±ng 1/(1-p)?**
- Äá»ƒ Ä‘áº£m báº£o expected value giá»‘ng nhau giá»¯a training vÃ  testing
- Training: E[y] = x * (1-p) * 1/(1-p) = x
- Testing: E[y] = x

---

### 4.7. Softmax Activation

**CÃ´ng thá»©c:**
```
softmax(x_i) = exp(x_i) / Î£(exp(x_j)) for j = 1 to n

VÃ­ dá»¥:
Input:  [2.0, 1.0, 0.1]
Output: [0.659, 0.242, 0.099]
```

**Äáº·c Ä‘iá»ƒm:**
- Output luÃ´n trong khoáº£ng [0, 1]
- Tá»•ng cÃ¡c output = 1 (phÃ¢n bá»‘ xÃ¡c suáº¥t)
- ThÃ­ch há»£p cho multi-class classification

---

### 4.8. Loss Function: Sparse Categorical Crossentropy

**CÃ´ng thá»©c:**
```
Loss = -log(p_{y_true})

Trong Ä‘Ã³:
- y_true: true label (0 hoáº·c 1)
- p_{y_true}: xÃ¡c suáº¥t dá»± Ä‘oÃ¡n cho class Ä‘Ãºng

VÃ­ dá»¥:
True label: 1 (Benign)
Predictions: [0.3, 0.7]  # [P(Attack), P(Benign)]
Loss = -log(0.7) = 0.357
```

**Vá»›i Class Weights:**
```
Weighted_Loss = w_{y_true} * (-log(p_{y_true}))

Trong Ä‘Ã³:
- w_{y_true}: weight cá»§a class Ä‘Ãºng
```

**Tá»•ng Loss cho toÃ n bá»™ dataset:**
```
Total_Loss = (1/N) * Î£(Loss_i) for i = 1 to N

Vá»›i Class Weights:
Total_Loss = (1/Î£w_i) * Î£(w_i * Loss_i)
```

---

### 4.9. Optimizer: Adam

**CÃ´ng thá»©c Adam (Adaptive Moment Estimation):**

```
Step 1: TÃ­nh gradient
g_t = âˆ‡L(Î¸_{t-1})

Step 2: TÃ­nh first moment (momentum)
m_t = Î²_1 * m_{t-1} + (1 - Î²_1) * g_t

Step 3: TÃ­nh second moment (RMSprop)
v_t = Î²_2 * v_{t-1} + (1 - Î²_2) * g_tÂ²

Step 4: Bias correction
mÌ‚_t = m_t / (1 - Î²_1^t)
vÌ‚_t = v_t / (1 - Î²_2^t)

Step 5: Update parameters
Î¸_t = Î¸_{t-1} - Î± * mÌ‚_t / (âˆšvÌ‚_t + Îµ)

Hyperparameters (máº·c Ä‘á»‹nh):
- Î± (learning rate): 0.001
- Î²_1: 0.9
- Î²_2: 0.999
- Îµ: 1e-7
```

**Táº¡i sao chá»n Adam?**
- Káº¿t há»£p momentum vÃ  RMSprop
- Adaptive learning rate cho tá»«ng parameter
- Hoáº¡t Ä‘á»™ng tá»‘t vá»›i sparse gradients
- Ãt cáº§n tune hyperparameters

---

## 5. QUÃ TRÃŒNH TIá»€N Xá»¬ LÃ Dá»® LIá»†U

### 5.1. Load vÃ  Merge Data

```python
# Load 20 CSV files
for file in csv_files:
    df_temp = pd.read_csv(file)
    dfs.append(df_temp)

# Merge táº¥t cáº£
df = pd.concat(dfs, ignore_index=True)
```

---

### 5.2. Label Mapping

**Chuyá»ƒn Ä‘á»•i Multi-class â†’ Binary:**

```python
def map_to_binary(label):
    if 'benign' in label.lower():
        return 'Benign'
    else:
        return 'Attack'

# Ãp dá»¥ng
df['binary_label'] = df['Label'].apply(map_to_binary)

# Encode thÃ nh sá»‘
LabelEncoder:
  'Attack' â†’ 0
  'Benign' â†’ 1
```

**Attack types bao gá»“m:**
- DDOS attacks (ICMP, UDP, TCP, SYN flood, ...)
- DOS attacks
- MIRAI botnet
- Scanning attacks (Port scan, OS scan, ...)
- Web attacks (SQL injection, XSS, ...)
- MITM attacks

---

### 5.3. Xá»­ LÃ½ Missing Values

```python
# Kiá»ƒm tra missing values
missing = X.isnull().sum().sum()

# Fill vá»›i 0
if missing > 0:
    X = X.fillna(0)
```

**Táº¡i sao fill báº±ng 0?**
- 0 lÃ  giÃ¡ trá»‹ neutral trong network traffic
- KhÃ´ng lÃ m sai lá»‡ch phÃ¢n bá»‘ sau normalization

---

### 5.4. Xá»­ LÃ½ Infinite Values

```python
# Thay tháº¿ inf vÃ  -inf
X = X.replace([np.inf, -np.inf], 0)
```

**Nguá»“n gá»‘c infinite values:**
- Chia cho 0 trong feature engineering
- Log cá»§a sá»‘ Ã¢m hoáº·c 0
- Overflow trong tÃ­nh toÃ¡n

---

### 5.5. Loáº¡i Bá» Constant Columns

```python
# TÃ¬m cÃ¡c cá»™t cÃ³ duy nháº¥t 1 giÃ¡ trá»‹
constant_cols = [col for col in X.columns if X[col].nunique() <= 1]

# Loáº¡i bá»
X = X.drop(constant_cols, axis=1)
```

**LÃ½ do:**
- Constant columns khÃ´ng mang thÃ´ng tin
- GÃ¢y lÃ£ng phÃ­ computational resources

---

### 5.6. Normalization: StandardScaler

**CÃ´ng thá»©c:**
```
x_scaled = (x - Î¼) / Ïƒ

Trong Ä‘Ã³:
- Î¼: mean cá»§a feature
- Ïƒ: standard deviation cá»§a feature
```

**VÃ­ dá»¥:**
```
Original: [100, 200, 300, 400, 500]
Î¼ = 300
Ïƒ = 141.42

Scaled: [-1.414, -0.707, 0, 0.707, 1.414]
```

**Lá»£i Ã­ch:**
- Mean = 0, Std = 1
- GiÃºp gradient descent há»™i tá»¥ nhanh hÆ¡n
- TrÃ¡nh features cÃ³ range lá»›n "dominate" model

**Quan trá»ng:**
```python
# Fit trÃªn training set
scaler.fit(X_train)

# Transform cáº£ 3 táº­p
X_train_scaled = scaler.transform(X_train)
X_val_scaled = scaler.transform(X_val)    # DÃ¹ng statistics tá»« train
X_test_scaled = scaler.transform(X_test)  # DÃ¹ng statistics tá»« train
```

**Táº¡i sao khÃ´ng fit trÃªn val/test?**
- TrÃ¡nh **data leakage**
- Model pháº£i há»c tá»« training distribution
- Testing pháº£i mÃ´ phá»ng real-world (khÃ´ng biáº¿t trÆ°á»›c statistics)

---

## 6. THUáº¬T TOÃN HUáº¤N LUYá»†N

### 6.1. Forward Propagation

**QuÃ¡ trÃ¬nh:**

```
Step 1: Input â†’ CNN Module
  x â†’ Reshape â†’ Conv1D â†’ BN â†’ Pool â†’ ... â†’ Flatten
  Output: CNN_features (1024 dimensions)

Step 2: Input â†’ GRU Module
  x â†’ Reshape â†’ GRU1 â†’ GRU2
  Output: GRU_features (64 dimensions)

Step 3: Concatenate
  Combined = [CNN_features, GRU_features]
  Output: (1088 dimensions)

Step 4: MLP Module
  Combined â†’ Dense â†’ BN â†’ Dropout â†’ Dense â†’ BN â†’ Dropout
  Output: Hidden_features (128 dimensions)

Step 5: Classification
  Hidden â†’ Dense(2) â†’ Softmax
  Output: [P(Attack), P(Benign)]
```

---

### 6.2. Loss Calculation

**Vá»›i Class Weights:**

```python
# Giáº£ sá»­ batch cÃ³ 4 samples:
y_true = [0, 1, 0, 0]  # Attack, Benign, Attack, Attack
y_pred = [[0.9, 0.1],  # Dá»± Ä‘oÃ¡n Ä‘Ãºng Attack
          [0.3, 0.7],  # Dá»± Ä‘oÃ¡n Ä‘Ãºng Benign
          [0.8, 0.2],  # Dá»± Ä‘oÃ¡n Ä‘Ãºng Attack
          [0.6, 0.4]]  # Dá»± Ä‘oÃ¡n Ä‘Ãºng Attack

# Class weights (vÃ­ dá»¥)
w_attack = 0.024
w_benign = 1.0

# Loss cho tá»«ng sample
loss[0] = w_attack * (-log(0.9)) = 0.024 * 0.105 = 0.00252
loss[1] = w_benign * (-log(0.7)) = 1.0 * 0.357 = 0.357
loss[2] = w_attack * (-log(0.8)) = 0.024 * 0.223 = 0.00535
loss[3] = w_attack * (-log(0.6)) = 0.024 * 0.511 = 0.01226

# Total loss
Total_Loss = mean(loss) = 0.094
```

**Nháº­n xÃ©t:**
- Sample Benign Ä‘Ã³ng gÃ³p loss lá»›n hÆ¡n (~97% total loss)
- Model sáº½ focus nhiá»u hÆ¡n vÃ o class Benign
- Giáº£i quyáº¿t imbalance problem

---

### 6.3. Backward Propagation

**CÃ´ng thá»©c Chain Rule:**

```
âˆ‚Loss/âˆ‚W = âˆ‚Loss/âˆ‚y * âˆ‚y/âˆ‚z * âˆ‚z/âˆ‚W

Trong Ä‘Ã³:
- y: output cá»§a layer
- z: input cá»§a activation function
- W: weights
```

**VÃ­ dá»¥ vá»›i Dense Layer:**

```
Layer: z = WÂ·x + b
Activation: y = ReLU(z)
Loss: L

âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚y * âˆ‚y/âˆ‚z * âˆ‚z/âˆ‚W
      = âˆ‚L/âˆ‚y * ReLU'(z) * x

Trong Ä‘Ã³:
ReLU'(z) = {
    1  if z > 0
    0  if z â‰¤ 0
}
```

---

### 6.4. Weight Update vá»›i Adam

```python
# Pseudo-code
for epoch in range(EPOCHS):
    for batch in training_data:
        # Forward pass
        predictions = model(batch_X)

        # Calculate loss vá»›i class weights
        loss = weighted_crossentropy(predictions, batch_y, class_weights)

        # Backward pass
        gradients = compute_gradients(loss)

        # Update weights vá»›i Adam
        optimizer.apply_gradients(gradients)
```

---

### 6.5. Training Loop Chi Tiáº¿t

```python
# Hyperparameters
EPOCHS = 50
BATCH_SIZE = 2048
LEARNING_RATE = 0.001

# Má»—i epoch
for epoch in range(EPOCHS):
    # Training phase
    for batch_idx in range(num_batches):
        # Láº¥y batch
        batch_X = X_train[batch_idx*BATCH_SIZE : (batch_idx+1)*BATCH_SIZE]
        batch_y = y_train[batch_idx*BATCH_SIZE : (batch_idx+1)*BATCH_SIZE]

        # Forward + Backward + Update
        # (Ä‘Æ°á»£c handle bá»Ÿi model.fit())

    # Validation phase
    val_loss = evaluate(X_val, y_val)
    val_accuracy = evaluate_accuracy(X_val, y_val)

    # Callbacks
    # - EarlyStopping: kiá»ƒm tra val_loss
    # - ReduceLROnPlateau: giáº£m learning rate náº¿u cáº§n
    # - ModelCheckpoint: lÆ°u best model
```

---

## 7. Xá»¬ LÃ IMBALANCED DATA

### 7.1. Váº¥n Äá» Imbalance

**PhÃ¢n bá»‘ dataset:**
```
Attack: 14,725,951 (97.66%)
Benign:    352,302 (2.34%)
Ratio: 41.80:1
```

**Há»‡ quáº£ náº¿u khÃ´ng xá»­ lÃ½:**
- Model sáº½ bias vá» class Attack
- Dá»± Ä‘oÃ¡n "táº¥t cáº£ lÃ  Attack" â†’ accuracy 97.66% nhÆ°ng vÃ´ dá»¥ng!
- Recall cá»§a Benign ráº¥t tháº¥p (nhiá»u False Negatives)

---

### 7.2. Giáº£i PhÃ¡p: Class Weights

**CÃ´ng thá»©c tÃ­nh Class Weight:**

```
w_i = n_samples / (n_classes * n_samples_i)

Trong Ä‘Ã³:
- n_samples: tá»•ng sá»‘ samples
- n_classes: sá»‘ lÆ°á»£ng classes
- n_samples_i: sá»‘ samples cá»§a class i
```

**Ãp dá»¥ng:**

```python
from sklearn.utils.class_weight import compute_class_weight

# TÃ­nh toÃ¡n
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)

# Káº¿t quáº£ (vÃ­ dá»¥):
# class 0 (Attack): w = 0.024
# class 1 (Benign): w = 1.000

# Benign Ä‘Æ°á»£c tÄƒng trá»ng sá»‘ ~42 láº§n so vá»›i Attack
```

**CÃ¡ch hoáº¡t Ä‘á»™ng:**

```
Sample Attack cÃ³ loss = 0.1
  â†’ Weighted loss = 0.024 * 0.1 = 0.0024

Sample Benign cÃ³ loss = 0.1
  â†’ Weighted loss = 1.0 * 0.1 = 0.1

â†’ Model quan tÃ¢m nhiá»u hÆ¡n Ä‘áº¿n Benign!
```

---

### 7.3. áº¢nh HÆ°á»Ÿng Äáº¿n Training

**KhÃ´ng cÃ³ Class Weights:**
```
Epoch 1: Accuracy = 97.5%, Recall(Benign) = 10%
Epoch 10: Accuracy = 98.0%, Recall(Benign) = 20%
â†’ Model chá»‰ há»c predict "Attack"
```

**CÃ³ Class Weights:**
```
Epoch 1: Accuracy = 95.0%, Recall(Benign) = 60%
Epoch 10: Accuracy = 98.5%, Recall(Benign) = 95%
â†’ Model há»c cÃ¢n báº±ng cáº£ 2 classes
```

---

## 8. CALLBACKS VÃ€ OPTIMIZATION

### 8.1. EarlyStopping

**Má»¥c Ä‘Ã­ch**: Dá»«ng training khi model khÃ´ng cÃ²n cáº£i thiá»‡n

**CÆ¡ cháº¿:**
```python
EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)
```

**Hoáº¡t Ä‘á»™ng:**

```
Epoch 1:  val_loss = 0.100 â†’ Best = 0.100, Counter = 0
Epoch 2:  val_loss = 0.090 â†’ Best = 0.090, Counter = 0
Epoch 3:  val_loss = 0.085 â†’ Best = 0.085, Counter = 0
...
Epoch 15: val_loss = 0.050 â†’ Best = 0.050, Counter = 0
Epoch 16: val_loss = 0.051 â†’ Best = 0.050, Counter = 1
Epoch 17: val_loss = 0.052 â†’ Best = 0.050, Counter = 2
...
Epoch 25: val_loss = 0.055 â†’ Best = 0.050, Counter = 10
â†’ STOP! Restore weights tá»« Epoch 15
```

**Lá»£i Ã­ch:**
- TrÃ¡nh overfitting
- Tiáº¿t kiá»‡m thá»i gian training
- Tá»± Ä‘á»™ng chá»n sá»‘ epochs tá»‘i Æ°u

---

### 8.2. ReduceLROnPlateau

**Má»¥c Ä‘Ã­ch**: Giáº£m learning rate khi model plateau (khÃ´ng cáº£i thiá»‡n)

**CÆ¡ cháº¿:**
```python
ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-7
)
```

**Hoáº¡t Ä‘á»™ng:**

```
Epoch 1-10:  val_loss giáº£m â†’ lr = 0.001
Epoch 11-15: val_loss khÃ´ng Ä‘á»•i â†’ Counter = 5 â†’ lr = 0.001 * 0.5 = 0.0005
Epoch 16-20: val_loss giáº£m tiáº¿p â†’ lr = 0.0005
Epoch 21-25: val_loss khÃ´ng Ä‘á»•i â†’ Counter = 5 â†’ lr = 0.0005 * 0.5 = 0.00025
...
```

**LÃ½ do:**
- Learning rate cao: Há»™i tá»¥ nhanh nhÆ°ng cÃ³ thá»ƒ bá» qua optimum
- Learning rate tháº¥p: Há»™i tá»¥ cháº­m nhÆ°ng chÃ­nh xÃ¡c hÆ¡n
- Adaptive LR: Káº¿t há»£p Æ°u Ä‘iá»ƒm cáº£ 2

---

### 8.3. ModelCheckpoint

**Má»¥c Ä‘Ã­ch**: LÆ°u model tá»‘t nháº¥t trong quÃ¡ trÃ¬nh training

**CÆ¡ cháº¿:**
```python
ModelCheckpoint(
    'best_model.h5',
    monitor='val_accuracy',
    save_best_only=True,
    verbose=1
)
```

**Hoáº¡t Ä‘á»™ng:**

```
Epoch 1:  val_acc = 0.95 â†’ Save (best so far)
Epoch 2:  val_acc = 0.96 â†’ Save (better)
Epoch 3:  val_acc = 0.95 â†’ Skip (worse)
Epoch 4:  val_acc = 0.97 â†’ Save (better)
...
Epoch 50: val_acc = 0.96 â†’ Skip

â†’ Final model = Epoch 4 model (val_acc = 0.97)
```

**Lá»£i Ã­ch:**
- KhÃ´ng lo máº¥t model tá»‘t nháº¥t náº¿u training bá»‹ lá»—i
- Tá»± Ä‘á»™ng chá»n model perform tá»‘t nháº¥t
- Backup an toÃ n

---

## 9. ÄÃNH GIÃ MÃ” HÃŒNH

### 9.1. Confusion Matrix

**Äá»‹nh nghÄ©a:**

```
                    Predicted
                Attack    Benign
Actual  Attack     TP        FN
        Benign     FP        TN

TP: True Positive  - Dá»± Ä‘oÃ¡n Attack, thá»±c táº¿ Attack âœ“
TN: True Negative  - Dá»± Ä‘oÃ¡n Benign, thá»±c táº¿ Benign âœ“
FP: False Positive - Dá»± Ä‘oÃ¡n Attack, thá»±c táº¿ Benign âœ— (Type I Error)
FN: False Negative - Dá»± Ä‘oÃ¡n Benign, thá»±c táº¿ Attack âœ— (Type II Error)
```

---

### 9.2. Metrics

**1. Accuracy (Äá»™ chÃ­nh xÃ¡c tá»•ng thá»ƒ)**

```
Accuracy = (TP + TN) / (TP + TN + FP + FN)

Ã nghÄ©a: Tá»· lá»‡ dá»± Ä‘oÃ¡n Ä‘Ãºng trÃªn tá»•ng sá»‘ máº«u
Pháº¡m vi: [0, 1], cao hÆ¡n = tá»‘t hÆ¡n
```

**VÃ­ dá»¥:**
```
TP=2900000, TN=68000, FP=2000, FN=13000
Accuracy = (2900000 + 68000) / 3000000 = 0.995 (99.5%)
```

---

**2. Precision (Äá»™ chÃ­nh xÃ¡c dÆ°Æ¡ng tÃ­nh)**

```
Precision = TP / (TP + FP)

Ã nghÄ©a: Trong cÃ¡c máº«u dá»± Ä‘oÃ¡n lÃ  Attack, cÃ³ bao nhiÃªu % Ä‘Ãºng?
CÃ¢u há»i: "Khi model bÃ¡o Attack, tÃ´i tin Ä‘Æ°á»£c bao nhiÃªu?"
```

**VÃ­ dá»¥:**
```
TP=2900000, FP=2000
Precision = 2900000 / (2900000 + 2000) = 0.9993 (99.93%)
```

---

**3. Recall (Äá»™ nháº¡y, Sensitivity, True Positive Rate)**

```
Recall = TP / (TP + FN)

Ã nghÄ©a: Trong cÃ¡c máº«u thá»±c táº¿ lÃ  Attack, model phÃ¡t hiá»‡n Ä‘Æ°á»£c bao nhiÃªu %?
CÃ¢u há»i: "Model bá» sÃ³t bao nhiÃªu attacks?"
```

**VÃ­ dá»¥:**
```
TP=2900000, FN=13000
Recall = 2900000 / (2900000 + 13000) = 0.9955 (99.55%)
```

**Quan trá»ng trong IDS:**
- Recall tháº¥p = Nhiá»u attacks bá»‹ bá» sÃ³t = Nguy hiá»ƒm!
- Trong IDS, Recall quan trá»ng hÆ¡n Precision

---

**4. F1-Score (Harmonic Mean cá»§a Precision vÃ  Recall)**

```
F1 = 2 * (Precision * Recall) / (Precision + Recall)

Ã nghÄ©a: Äiá»ƒm cÃ¢n báº±ng giá»¯a Precision vÃ  Recall
Pháº¡m vi: [0, 1], cao hÆ¡n = tá»‘t hÆ¡n
```

**VÃ­ dá»¥:**
```
Precision = 0.9993, Recall = 0.9955
F1 = 2 * (0.9993 * 0.9955) / (0.9993 + 0.9955) = 0.9974 (99.74%)
```

**Táº¡i sao dÃ¹ng Harmonic Mean?**
```
Arithmetic Mean = (0.9993 + 0.9955) / 2 = 0.9974
Harmonic Mean = 2 * (0.9993 * 0.9955) / (0.9993 + 0.9955) = 0.9974

NhÆ°ng vá»›i sá»‘ liá»‡u khÃ¡c:
Precision = 1.0, Recall = 0.1
Arithmetic = 0.55 (misleading!)
Harmonic = 0.18 (pháº£n Ã¡nh Ä‘Ãºng model kÃ©m)
```

---

### 9.3. ÄÃ¡nh GiÃ¡ Cho Binary Classification

**Äá»‘i vá»›i class Attack (class 0):**
```
Precision_Attack = TP_attack / (TP_attack + FP_attack)
Recall_Attack = TP_attack / (TP_attack + FN_attack)
F1_Attack = 2 * P * R / (P + R)
```

**Äá»‘i vá»›i class Benign (class 1):**
```
Precision_Benign = TN / (TN + FN)
Recall_Benign = TN / (TN + FP)
F1_Benign = 2 * P * R / (P + R)
```

**Overall Metrics:**
```
Macro-average: Trung bÃ¬nh khÃ´ng trá»ng sá»‘
  F1_macro = (F1_Attack + F1_Benign) / 2

Weighted-average: Trung bÃ¬nh cÃ³ trá»ng sá»‘
  F1_weighted = (n_attack * F1_Attack + n_benign * F1_Benign) / n_total
```

---

## 10. Káº¾T LUáº¬N

### 10.1. Æ¯u Äiá»ƒm Cá»§a Kiáº¿n TrÃºc CNN-GRU

**1. Káº¿t há»£p 2 loáº¡i features:**
- CNN: Spatial patterns (Ä‘áº·c trÆ°ng cá»¥c bá»™)
- GRU: Temporal patterns (Ä‘áº·c trÆ°ng thá»i gian)

**2. Hiá»‡u quáº£ vá»›i dá»¯ liá»‡u IoT:**
- Network traffic cÃ³ cáº£ spatial vÃ  temporal characteristics
- CNN phÃ¡t hiá»‡n attack signatures
- GRU phÃ¡t hiá»‡n attack sequences

**3. Performance cao:**
- Accuracy > 99%
- Recall > 98% (Ã­t bá» sÃ³t attacks)
- Training time há»£p lÃ½ vá»›i GPU

---

### 10.2. CÃ¡c Ká»¹ Thuáº­t Quan Trá»ng

**1. Xá»­ lÃ½ Imbalanced Data:**
- Class Weights â†’ Giáº£i quyáº¿t ratio 42:1
- KhÃ´ng cáº§n SMOTE/undersampling

**2. Regularization:**
- BatchNormalization â†’ á»”n Ä‘á»‹nh training
- Dropout (0.5, 0.3) â†’ TrÃ¡nh overfitting
- EarlyStopping â†’ Dá»«ng Ä‘Ãºng lÃºc

**3. Optimization:**
- Adam optimizer â†’ Adaptive learning rate
- ReduceLROnPlateau â†’ Fine-tuning
- Class Weights â†’ Focus vÃ o minority class

---

### 10.3. Äiá»ƒm Máº¡nh So Vá»›i CÃ¡c PhÆ°Æ¡ng PhÃ¡p KhÃ¡c

**So vá»›i Traditional ML (Random Forest, SVM):**
- âœ“ Tá»± Ä‘á»™ng feature extraction
- âœ“ Xá»­ lÃ½ Ä‘Æ°á»£c dá»¯ liá»‡u phá»©c táº¡p
- âœ“ Capture Ä‘Æ°á»£c temporal dependencies

**So vá»›i Simple DNN:**
- âœ“ CNN giáº£m sá»‘ parameters
- âœ“ GRU xá»­ lÃ½ sequences tá»‘t hÆ¡n
- âœ“ Ãt overfitting hÆ¡n

**So vá»›i LSTM:**
- âœ“ GRU nhanh hÆ¡n (Ã­t parameters)
- âœ“ Hiá»‡u quáº£ tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i dataset lá»›n
- âœ“ Dá»… train hÆ¡n

---

### 10.4. Khuyáº¿n Nghá»‹

**Äá»ƒ cáº£i thiá»‡n thÃªm:**

1. **Feature Engineering:**
   - ThÃªm domain-specific features
   - Feature selection Ä‘á»ƒ giáº£m dimensions

2. **Model Ensemble:**
   - Káº¿t há»£p nhiá»u models
   - Voting hoáº·c stacking

3. **Hyperparameter Tuning:**
   - Grid Search / Random Search
   - Bayesian Optimization

4. **Data Augmentation:**
   - Synthetic minority oversampling
   - Adversarial training

---

### 10.5. Workflow Tá»•ng QuÃ¡t

```
1. DATA PREPARATION
   â”œâ”€ Load CSV files
   â”œâ”€ Merge datasets
   â”œâ”€ Handle missing/infinite values
   â”œâ”€ Binary label mapping
   â””â”€ Train/Val/Test split (70/10/20)

2. PREPROCESSING
   â”œâ”€ Remove constant columns
   â”œâ”€ StandardScaler normalization
   â””â”€ Calculate class weights

3. MODEL BUILDING
   â”œâ”€ CNN Module (3 Conv blocks)
   â”œâ”€ GRU Module (2 GRU layers)
   â”œâ”€ Concatenate
   â””â”€ MLP Module (2 Dense + Dropout)

4. TRAINING
   â”œâ”€ Adam optimizer (lr=0.001)
   â”œâ”€ Sparse Categorical Crossentropy + Class Weights
   â”œâ”€ Callbacks: EarlyStopping, ReduceLR, Checkpoint
   â””â”€ 50 epochs, batch_size=2048

5. EVALUATION
   â”œâ”€ Confusion Matrix
   â”œâ”€ Accuracy, Precision, Recall, F1-Score
   â””â”€ Classification Report

6. DEPLOYMENT
   â”œâ”€ Save model (.h5)
   â”œâ”€ Save scaler (.pkl)
   â”œâ”€ Save label encoder (.pkl)
   â””â”€ Documentation
```

---

## PHá»¤ Lá»¤C: Báº¢NG TÃ“M Táº®T HYPERPARAMETERS

| Component | Hyperparameter | Value | LÃ½ do chá»n |
|-----------|---------------|-------|------------|
| **CNN** | Filters | 64, 128, 256 | TÄƒng dáº§n Ä‘á»ƒ capture complex patterns |
| | Kernel size | 3 | CÃ¢n báº±ng receptive field vÃ  computation |
| | Pooling | MaxPool(2) | Giáº£m dimensions 50% |
| **GRU** | Units | 128, 64 | Äá»§ lá»›n Ä‘á»ƒ capture temporal dependencies |
| | Layers | 2 | CÃ¢n báº±ng capacity vÃ  overfitting |
| **MLP** | Units | 256, 128 | Giáº£m dáº§n Ä‘á»ƒ extract high-level features |
| | Dropout | 0.5, 0.3 | Regularization máº¡nh á»Ÿ layer Ä‘áº§u |
| **Training** | Epochs | 50 | Theo paper DeepFed |
| | Batch size | 2048 | Tá»‘i Æ°u cho GPU 8GB |
| | Learning rate | 0.001 | Adam default, hiá»‡u quáº£ vá»›i dataset lá»›n |
| **Callbacks** | EarlyStopping patience | 10 | Cho phÃ©p recover tá»« temporary plateaus |
| | ReduceLR patience | 5 | Nhanh hÆ¡n EarlyStopping |
| | ReduceLR factor | 0.5 | Giáº£m LR 50% má»—i láº§n |

---

## TÃ€I LIá»†U THAM KHáº¢O

1. **DeepFed Paper**: Federated Learning Architecture for IoT IDS
2. **GRU Paper**: Cho et al. (2014) - "Learning Phrase Representations using RNN Encoder-Decoder"
3. **Adam Optimizer**: Kingma & Ba (2014) - "Adam: A Method for Stochastic Optimization"
4. **Batch Normalization**: Ioffe & Szegedy (2015) - "Batch Normalization: Accelerating Deep Network Training"
5. **Dropout**: Srivastava et al. (2014) - "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"

---

**Â© 2025 - IoT Intrusion Detection System**
**CNN-GRU Deep Learning Model**
