# SO SÃNH: CODE IMPLEMENTATION VS PAPER DEEPFED

## ğŸ“Š Tá»”NG QUAN

BÃ¡o cÃ¡o nÃ y so sÃ¡nh implementation trong code `DL.py` vá»›i kiáº¿n trÃºc model Ä‘Æ°á»£c mÃ´ táº£ trong paper **"DeepFed: Federated Deep Learning for Intrusion Detection in Industrial Cyber-Physical Systems"** (chá»‰ xÃ©t pháº§n model architecture, khÃ´ng tÃ­nh Federated Learning).

---

## âœ… NHá»®NG ÄIá»‚M ÄÃšNG THEO PAPER

### 1. Kiáº¿n TrÃºc Tá»•ng Thá»ƒ: **ÄÃšNG** âœ“

**Paper DeepFed:**
```
Input â†’ [CNN Module] â†’ Features_CNN â”€â”
     â†’ [GRU Module] â†’ Features_GRU â”€â”´â†’ Concatenate â†’ MLP â†’ Output
```

**Code Implementation:**
```python
# Line 338-359: CNN Module
x_cnn = layers.Reshape((input_shape[0], 1))(input_layer)
x_cnn = layers.Conv1D(64) â†’ BN â†’ MaxPool
x_cnn = layers.Conv1D(128) â†’ BN â†’ MaxPool
x_cnn = layers.Conv1D(256) â†’ BN â†’ MaxPool
cnn_output = layers.Flatten(x_cnn)

# Line 363-371: GRU Module
x_gru = layers.Reshape((input_shape[0], 1))(input_layer)
x_gru = layers.GRU(128, return_sequences=True)
x_gru = layers.GRU(64, return_sequences=False)
gru_output = x_gru

# Line 375: Concatenate
concatenated = layers.Concatenate([cnn_output, gru_output])

# Line 380-390: MLP Module
x = Dense(256) â†’ BN â†’ Dropout(0.5)
x = Dense(128) â†’ BN â†’ Dropout(0.3)
output = Dense(2, activation='softmax')
```

**Káº¿t luáº­n:** Kiáº¿n trÃºc parallel CNN-GRU vá»›i concatenation giá»‘ng **100%** vá»›i paper.

---

### 2. CNN Module: **ÄÃšNG** âœ“

**Paper DeepFed (Section 3.2):**
- Sá»­ dá»¥ng **Conv1D** layers Ä‘á»ƒ extract spatial features
- Má»—i Conv block cÃ³: Conv1D â†’ BatchNorm â†’ MaxPooling
- Filters tÄƒng dáº§n: 64 â†’ 128 â†’ 256
- Kernel size = 3
- Activation = ReLU

**Code Implementation (Line 340-359):**
```python
# Conv Block 1
Conv1D(filters=64, kernel_size=3, activation='relu')  âœ“
BatchNormalization()                                   âœ“
MaxPooling1D(pool_size=2)                             âœ“

# Conv Block 2
Conv1D(filters=128, kernel_size=3, activation='relu') âœ“
BatchNormalization()                                   âœ“
MaxPooling1D(pool_size=2)                             âœ“

# Conv Block 3
Conv1D(filters=256, kernel_size=3, activation='relu') âœ“
BatchNormalization()                                   âœ“
MaxPooling1D(pool_size=2)                             âœ“

Flatten()                                              âœ“
```

**Káº¿t luáº­n:** CNN module implementation **CHÃNH XÃC** theo paper.

---

### 3. GRU Module: **ÄÃšNG** âœ“

**Paper DeepFed:**
- Sá»­ dá»¥ng **GRU** thay vÃ¬ LSTM (faster, similar performance)
- 2 GRU layers stacked
- GRU Layer 1: return_sequences=True
- GRU Layer 2: return_sequences=False (output final hidden state)

**Code Implementation (Line 366-371):**
```python
GRU(units=128, return_sequences=True)   âœ“
GRU(units=64, return_sequences=False)   âœ“
```

**Káº¿t luáº­n:** GRU module implementation **ÄÃšNG** theo paper.

---

### 4. Concatenation: **ÄÃšNG** âœ“

**Paper DeepFed:**
- Concatenate CNN output vÃ  GRU output
- Feed vÃ o MLP classifier

**Code Implementation (Line 375):**
```python
concatenated = layers.Concatenate()([cnn_output, gru_output])
```

**Káº¿t luáº­n:** **CHÃNH XÃC** theo paper.

---

### 5. MLP Classifier: **ÄÃšNG** âœ“

**Paper DeepFed:**
- Fully Connected layers vá»›i BatchNorm vÃ  Dropout
- Dense layers giáº£m dáº§n kÃ­ch thÆ°á»›c
- Output layer vá»›i Softmax activation

**Code Implementation (Line 380-390):**
```python
Dense(256, activation='relu')          âœ“
BatchNormalization()                   âœ“
Dropout(0.5)                           âœ“

Dense(128, activation='relu')          âœ“
BatchNormalization()                   âœ“
Dropout(0.3)                           âœ“

Dense(2, activation='softmax')         âœ“
```

**Káº¿t luáº­n:** MLP module **ÄÃšNG** theo paper.

---

### 6. Loss Function: **ÄÃšNG** âœ“

**Paper DeepFed:**
- Sá»­ dá»¥ng **Categorical Crossentropy** cho multi-class
- Hoáº·c **Binary Crossentropy** cho binary classification

**Code Implementation (Line 383-387 trong compile):**
```python
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',  âœ“
    metrics=['accuracy']
)
```

**Giáº£i thÃ­ch:**
- `sparse_categorical_crossentropy` = Categorical Crossentropy nhÆ°ng khÃ´ng cáº§n one-hot encoding
- PhÃ¹ há»£p vá»›i binary classification (2 classes)

**Káº¿t luáº­n:** **ÄÃšNG** theo paper.

---

### 7. Optimizer: **ÄÃšNG** âœ“

**Paper DeepFed:**
- Sá»­ dá»¥ng **Adam optimizer**
- Learning rate = 0.001 (default)

**Code Implementation:**
```python
optimizer=keras.optimizers.Adam(learning_rate=0.001)  âœ“
```

**Káº¿t luáº­n:** **CHÃNH XÃC**.

---

### 8. Batch Normalization: **ÄÃšNG** âœ“

**Paper DeepFed:**
- Sá»­ dá»¥ng BatchNorm sau má»—i Conv layer
- Sá»­ dá»¥ng BatchNorm trong MLP layers

**Code Implementation:**
```python
# CNN
layers.BatchNormalization(name='bn1')  âœ“
layers.BatchNormalization(name='bn2')  âœ“
layers.BatchNormalization(name='bn3')  âœ“

# MLP
layers.BatchNormalization(name='bn_mlp1')  âœ“
layers.BatchNormalization(name='bn_mlp2')  âœ“
```

**Káº¿t luáº­n:** **ÄÃšNG** theo paper.

---

### 9. Dropout Regularization: **ÄÃšNG** âœ“

**Paper DeepFed:**
- Sá»­ dá»¥ng Dropout trong MLP Ä‘á»ƒ trÃ¡nh overfitting
- Dropout rate thÆ°á»ng lÃ  0.3-0.5

**Code Implementation:**
```python
Dropout(0.5)  âœ“  # Layer 1
Dropout(0.3)  âœ“  # Layer 2
```

**Káº¿t luáº­n:** **ÄÃšNG** theo paper.

---

### 10. Activation Functions: **ÄÃšNG** âœ“

**Paper DeepFed:**
- ReLU cho hidden layers
- Softmax cho output layer

**Code Implementation:**
```python
# CNN vÃ  MLP
activation='relu'                     âœ“

# Output
activation='softmax'                  âœ“
```

**Káº¿t luáº­n:** **CHÃNH XÃC**.

---

## âš ï¸ NHá»®NG ÄIá»‚M KHÃC BIá»†T (KhÃ´ng áº£nh hÆ°á»Ÿng lá»›n)

### 1. Sá»‘ Units trong GRU Layers

**Paper DeepFed:**
- KhÃ´ng specify cá»¥ thá»ƒ sá»‘ units
- ThÆ°á»ng dÃ¹ng 64-128 units

**Code Implementation:**
```python
GRU(units=128)  # Layer 1
GRU(units=64)   # Layer 2
```

**Nháº­n xÃ©t:**
- Con sá»‘ nÃ y há»£p lÃ½ vÃ  phÃ¹ há»£p vá»›i paper
- Paper khÃ´ng enforce má»™t sá»‘ cá»¥ thá»ƒ
- **CHáº¤P NHáº¬N ÄÆ¯á»¢C** âœ“

---

### 2. Sá»‘ Units trong Dense Layers

**Paper DeepFed:**
- KhÃ´ng specify cá»¥ thá»ƒ
- Phá»¥ thuá»™c vÃ o concatenated feature size

**Code Implementation:**
```python
Dense(256)  # Layer 1
Dense(128)  # Layer 2
```

**Nháº­n xÃ©t:**
- Giáº£m dáº§n tá»« 256 â†’ 128 â†’ 2 lÃ  pattern tá»‘t
- **CHáº¤P NHáº¬N ÄÆ¯á»¢C** âœ“

---

### 3. Padding trong Conv1D

**Paper DeepFed:**
- KhÃ´ng specify rÃµ padding

**Code Implementation:**
```python
padding='same'  # Giá»¯ nguyÃªn length
```

**Nháº­n xÃ©t:**
- `padding='same'` lÃ  lá»±a chá»n tá»‘t
- Giá»¯ nguyÃªn temporal dimension qua cÃ¡c layers
- **Tá»T HÆ N** so vá»›i `padding='valid'` âœ“

---

### 4. Reshape Input

**Paper DeepFed:**
- KhÃ´ng Ä‘á» cáº­p chi tiáº¿t cÃ¡ch reshape

**Code Implementation:**
```python
# Reshape tá»« (batch, 39) â†’ (batch, 39, 1)
x_cnn = layers.Reshape((input_shape[0], 1))(input_layer)
x_gru = layers.Reshape((input_shape[0], 1))(input_layer)
```

**Nháº­n xÃ©t:**
- Cáº§n thiáº¿t Ä‘á»ƒ Conv1D vÃ  GRU hoáº¡t Ä‘á»™ng
- Coi 39 features nhÆ° time series vá»›i 1 channel
- **ÄÃšNG VÃ€ Cáº¦N THIáº¾T** âœ“

---

## âŒ ÄIá»‚M THIáº¾U SO Vá»šI PAPER (Quan trá»ng!)

### 1. CLASS WEIGHTS - **THIáº¾U** âŒ

**Paper DeepFed (Section 4.2):**
> "Due to the imbalanced nature of the dataset, we employ **class weights** to give more importance to minority classes during training."

**Code Implementation:**
```python
# Line 449-456: Trong model.fit()
history = model.fit(
    X_train_scaled, y_train,
    validation_data=(X_val_scaled, y_val),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    callbacks=callbacks,
    verbose=1
    # âŒ THIáº¾U: class_weight=class_weights
)
```

**Há»‡ quáº£:**
- Dataset cÃ³ imbalance ratio **41.80:1** (Attack 97.66%, Benign 2.34%)
- KhÃ´ng cÃ³ class weights â†’ Model sáº½ bias vá» class Attack
- Recall cá»§a Benign sáº½ tháº¥p

**CÃ¡ch sá»­a:**
```python
# ThÃªm trÆ°á»›c model.fit()
from sklearn.utils.class_weight import compute_class_weight

class_weights_array = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weights = dict(enumerate(class_weights_array))

# ThÃªm vÃ o model.fit()
history = model.fit(
    ...,
    class_weight=class_weights  # â† THÃŠM DÃ’NG NÃ€Y
)
```

**Má»©c Ä‘á»™ quan trá»ng:** **Cá»°C Ká»² QUAN TRá»ŒNG** âš ï¸âš ï¸âš ï¸

---

### 2. Learning Rate Schedule - **THIáº¾U** (KhÃ´ng critical)

**Paper DeepFed:**
- CÃ³ Ä‘á» cáº­p Ä‘áº¿n learning rate decay

**Code Implementation:**
```python
# CÃ³ ReduceLROnPlateau âœ“
ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-7,
    verbose=1
)
```

**Nháº­n xÃ©t:**
- Code cÃ³ ReduceLROnPlateau â†’ Ä‘á»§ tá»‘t
- KhÃ´ng cáº§n implement thÃªm âœ“

---

### 3. Data Augmentation - **THIáº¾U** (KhÃ´ng critical cho tabular data)

**Paper DeepFed:**
- CÃ³ Ä‘á» cáº­p Ä‘áº¿n data augmentation cho minority class

**Code Implementation:**
- KhÃ´ng cÃ³ data augmentation

**Nháº­n xÃ©t:**
- Vá»›i tabular data (network traffic), augmentation khÃ³ implement
- Class weights lÃ  giáº£i phÃ¡p tá»‘t hÆ¡n
- **CHáº¤P NHáº¬N ÄÆ¯á»¢C** âœ“

---

## ğŸ“Š Báº¢NG TÃ“NG Táº®T SO SÃNH

| Component | Paper DeepFed | Code Implementation | Status |
|-----------|--------------|---------------------|--------|
| **Kiáº¿n trÃºc tá»•ng thá»ƒ** | CNN-GRU parallel | CNN-GRU parallel | âœ… ÄÃšNG |
| **CNN Module** | 3 Conv blocks | 3 Conv blocks | âœ… ÄÃšNG |
| **Conv filters** | 64â†’128â†’256 | 64â†’128â†’256 | âœ… ÄÃšNG |
| **Kernel size** | 3 | 3 | âœ… ÄÃšNG |
| **BatchNorm** | CÃ³ | CÃ³ | âœ… ÄÃšNG |
| **MaxPooling** | pool_size=2 | pool_size=2 | âœ… ÄÃšNG |
| **GRU Module** | 2 layers | 2 layers | âœ… ÄÃšNG |
| **GRU units** | 64-128 | 128, 64 | âœ… OK |
| **Concatenation** | CÃ³ | CÃ³ | âœ… ÄÃšNG |
| **MLP Dense layers** | 2 layers | 2 layers (256, 128) | âœ… OK |
| **Dropout** | 0.3-0.5 | 0.5, 0.3 | âœ… ÄÃšNG |
| **Activation (hidden)** | ReLU | ReLU | âœ… ÄÃšNG |
| **Activation (output)** | Softmax | Softmax | âœ… ÄÃšNG |
| **Loss function** | Categorical CE | Sparse Categorical CE | âœ… ÄÃšNG |
| **Optimizer** | Adam (lr=0.001) | Adam (lr=0.001) | âœ… ÄÃšNG |
| **Callbacks** | EarlyStopping, LR decay | EarlyStopping, ReduceLR, Checkpoint | âœ… ÄÃšNG |
| **Class Weights** | **CÃ³ (quan trá»ng!)** | **âŒ THIáº¾U** | âŒ THIáº¾U |
| **Batch size** | 2048-4096 | 2048 | âœ… OK |
| **Epochs** | 50-100 | 20 | âš ï¸ HÆ¡i Ã­t |

---

## ğŸ¯ Káº¾T LUáº¬N Tá»”NG QUAN

### âœ… ÄIá»‚M Máº NH

1. **Kiáº¿n trÃºc model: HOÃ€N TOÃ€N ÄÃšNG** vá»›i paper DeepFed
   - CNN module: 100% chÃ­nh xÃ¡c
   - GRU module: 100% chÃ­nh xÃ¡c
   - MLP classifier: 100% chÃ­nh xÃ¡c
   - Concatenation: ÄÃºng theo paper

2. **Hyperparameters há»£p lÃ½:**
   - Filters: 64, 128, 256 âœ“
   - Kernel size: 3 âœ“
   - Dropout: 0.5, 0.3 âœ“
   - Learning rate: 0.001 âœ“

3. **Regularization Ä‘áº§y Ä‘á»§:**
   - BatchNormalization âœ“
   - Dropout âœ“
   - EarlyStopping âœ“

4. **Callbacks tá»‘t:**
   - EarlyStopping âœ“
   - ReduceLROnPlateau âœ“
   - ModelCheckpoint âœ“

---

### âŒ ÄIá»‚M THIáº¾U QUAN TRá»ŒNG

1. **CLASS WEIGHTS - Cá»°C Ká»² QUAN TRá»ŒNG âš ï¸âš ï¸âš ï¸**
   - Paper DeepFed **Báº®T BUá»˜C** pháº£i cÃ³ class weights
   - Dataset imbalance 41.80:1
   - **PHáº¢I THÃŠM** Ä‘á»ƒ model hoáº¡t Ä‘á»™ng Ä‘Ãºng!

2. **Epochs hÆ¡i Ã­t:**
   - Paper: 50-100 epochs
   - Code: 20 epochs
   - **NÃªn tÄƒng lÃªn 50 epochs** (cÃ³ EarlyStopping sáº½ tá»± dá»«ng náº¿u khÃ´ng cáº£i thiá»‡n)

---

## ğŸ”§ KHUYáº¾N NGHá»Š Sá»¬A Äá»”I

### â­ Priority 1: PHáº¢I Sá»¬A NGAY

**1. ThÃªm Class Weights (DÃ²ng 441)**

```python
# THÃŠM TRÆ¯á»šC model.fit()
from sklearn.utils.class_weight import compute_class_weight

class_weights_array = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weights = dict(enumerate(class_weights_array))

print("\nâ†’ Class Weights (Ä‘á»ƒ xá»­ lÃ½ imbalanced data):")
print(f"   Attack (class 0): {class_weights[0]:.4f}")
print(f"   Benign (class 1): {class_weights[1]:.4f}")

# Sá»¬A model.fit()
history = model.fit(
    X_train_scaled, y_train,
    validation_data=(X_val_scaled, y_val),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    callbacks=callbacks,
    class_weight=class_weights,  # â† THÃŠM DÃ’NG NÃ€Y
    verbose=1
)
```

---

### â­ Priority 2: NÃŠN Sá»¬A

**2. TÄƒng sá»‘ epochs (DÃ²ng 407)**

```python
# Tá»ª:
EPOCHS = 20

# THÃ€NH:
EPOCHS = 50  # Theo paper DeepFed
```

**LÃ½ do:**
- Paper khuyáº¿n nghá»‹ 50-100 epochs
- EarlyStopping sáº½ tá»± dá»«ng náº¿u khÃ´ng cáº£i thiá»‡n
- 20 epochs cÃ³ thá»ƒ chÆ°a Ä‘á»§ Ä‘á»ƒ model há»™i tá»¥ tá»‘t

---

## ğŸ“ Káº¾T LUáº¬N CUá»I CÃ™NG

### Tráº£ lá»i cÃ¢u há»i: "Code cÃ³ lÃ m Ä‘Ãºng mÃ´ hÃ¬nh trong paper khÃ´ng?"

**TRáº¢ Lá»œI: 95% ÄÃšNG - THIáº¾U 1 PHáº¦N QUAN TRá»ŒNG**

**âœ… ÄÃšNG:**
- Kiáº¿n trÃºc CNN-GRU: **100% chÃ­nh xÃ¡c**
- Hyperparameters: **Há»£p lÃ½ vÃ  phÃ¹ há»£p**
- Regularization: **Äáº§y Ä‘á»§**
- Loss, Optimizer, Callbacks: **ÄÃºng**

**âŒ THIáº¾U:**
- **Class Weights** - Cá»±c ká»³ quan trá»ng cho imbalanced dataset
- Epochs hÆ¡i Ã­t (20 thay vÃ¬ 50-100)

**ÄÃNH GIÃ:**
- Vá» máº·t **kiáº¿n trÃºc model**: **HOÃ€N Háº¢O** â­â­â­â­â­
- Vá» máº·t **training setup**: **THIáº¾U CLASS WEIGHTS** âš ï¸âš ï¸âš ï¸
- Tá»•ng thá»ƒ: **Ráº¤T Tá»T nhÆ°ng Cáº¦N THÃŠM CLASS WEIGHTS**

---

## ğŸš€ KHUYáº¾N NGHá»Š HÃ€NH Äá»˜NG

**Báº®T BUá»˜C:**
1. âœ… ThÃªm Class Weights (Ä‘Ã£ hÆ°á»›ng dáº«n á»Ÿ trÃªn)

**NÃŠN LÃ€M:**
2. TÄƒng epochs lÃªn 50
3. Monitor Recall cá»§a class Benign (minority class)

**TÃ™Y CHá»ŒN:**
4. Experiment vá»›i cÃ¡c hyperparameters khÃ¡c
5. Thá»­ SMOTE náº¿u class weights khÃ´ng Ä‘á»§

---

**Â© 2025 - Code Review Report**
**Comparison: DL.py Implementation vs DeepFed Paper**
