# T√ÄI LI·ªÜU GI·∫¢I TH√çCH CHI TI·∫æT CODE CNN-GRU

## üìå T·ªîNG QUAN D·ª∞ √ÅN

### M·ª•c ƒë√≠ch
X√¢y d·ª±ng m√¥ h√¨nh Deep Learning ƒë·ªÉ ph√°t hi·ªán c√°c cu·ªôc t·∫•n c√¥ng m·∫°ng (Intrusion Detection) trong h·ªá th·ªëng IoT (Internet of Things) s·ª≠ d·ª•ng ki·∫øn tr√∫c CNN-GRU hybrid.

### Ki·∫øn tr√∫c Model
- **CNN (Convolutional Neural Network)**: Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng kh√¥ng gian (spatial features)
- **GRU (Gated Recurrent Unit)**: H·ªçc c√°c m·∫´u tu·∫ßn t·ª± (sequential patterns)
- **MLP (Multi-Layer Perceptron)**: K·∫øt h·ª£p features v√† ph√¢n lo·∫°i

### Lo·∫°i b√†i to√°n
**Binary Classification**: Ph√¢n lo·∫°i nh·ªã ph√¢n
- Class 0: **Benign** (Traffic b√¨nh th∆∞·ªùng, l√†nh t√≠nh)
- Class 1: **Attack** (Traffic t·∫•n c√¥ng)

---

## üìö C·∫§U TR√öC CODE CHI TI·∫æT

### PH·∫¶N 1: IMPORT TH∆Ø VI·ªÜN

```python
import pandas as pd              # X·ª≠ l√Ω d·ªØ li·ªáu d·∫°ng b·∫£ng
import numpy as np               # T√≠nh to√°n s·ªë h·ªçc, x·ª≠ l√Ω m·∫£ng
import matplotlib.pyplot as plt  # V·∫Ω bi·ªÉu ƒë·ªì
import seaborn as sns           # V·∫Ω bi·ªÉu ƒë·ªì ƒë·∫πp h∆°n
from sklearn.model_selection import train_test_split  # Chia train/val/test
from sklearn.preprocessing import StandardScaler, LabelEncoder  # Chu·∫©n h√≥a d·ªØ li·ªáu
from sklearn.metrics import *    # C√°c metrics ƒë√°nh gi√°
import tensorflow as tf          # Framework Deep Learning
from tensorflow import keras     # High-level API c·ªßa TensorFlow
```

**Gi·∫£i th√≠ch**:
- **pandas**: L√†m vi·ªác v·ªõi file CSV, x·ª≠ l√Ω DataFrame
- **numpy**: T√≠nh to√°n ma tr·∫≠n, m·∫£ng, c√°c ph√©p to√°n s·ªë h·ªçc
- **matplotlib & seaborn**: T·∫°o c√°c bi·ªÉu ƒë·ªì tr·ª±c quan
- **sklearn**: C√¥ng c·ª• Machine Learning (preprocessing, metrics)
- **tensorflow/keras**: X√¢y d·ª±ng v√† train Deep Learning model

---

### PH·∫¶N 2: THI·∫æT L·∫¨P BAN ƒê·∫¶U

```python
warnings.filterwarnings('ignore')  # T·∫Øt c√°c c·∫£nh b√°o kh√¥ng c·∫ßn thi·∫øt

# Set random seed ƒë·ªÉ k·∫øt qu·∫£ reproducible (c√≥ th·ªÉ t√°i t·∫°o)
np.random.seed(42)
tf.random.set_seed(42)

# Thi·∫øt l·∫≠p style cho bi·ªÉu ƒë·ªì
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
```

**T·∫°i sao c·∫ßn random seed?**
- ƒê·∫£m b·∫£o k·∫øt qu·∫£ gi·ªëng nhau m·ªói l·∫ßn ch·∫°y
- Quan tr·ªçng cho nghi√™n c·ª©u khoa h·ªçc, debug
- S·ªë 42 l√† convention (t·ª´ cu·ªën "The Hitchhiker's Guide to the Galaxy")

---

## üîÑ QUY TR√åNH X·ª¨ L√ù D·ªÆ LI·ªÜU (DATA PIPELINE)

### STEP 1: LOAD V√Ä PH√ÇN T√çCH DATASET

#### 1.1. T√¨m t·∫•t c·∫£ file CSV

```python
DATA_PATH = './IoT_Dataset_2023'
csv_files = []
for root, dirs, files in os.walk(DATA_PATH):
    for file in files:
        if file.endswith('.csv'):
            csv_files.append(os.path.join(root, file))
```

**Gi·∫£i th√≠ch**:
- `os.walk()`: Duy·ªát qua t·∫•t c·∫£ th∆∞ m·ª•c con
- T√¨m t·∫•t c·∫£ file c√≥ ƒëu√¥i `.csv`
- L∆∞u ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß v√†o list `csv_files`

**Output**: T√¨m th·∫•y 63 file CSV

#### 1.2. Load t·ª´ng file CSV

```python
dfs = []
for file in csv_files:
    try:
        df_temp = pd.read_csv(file)
        dfs.append(df_temp)
        print(f"‚úì Loaded: {os.path.basename(file):50s} - {len(df_temp):>10,} samples")
    except Exception as e:
        print(f"‚úó Error loading {file}: {e}")
```

**Gi·∫£i th√≠ch**:
- ƒê·ªçc t·ª´ng file CSV th√†nh DataFrame
- `try-except`: B·∫Øt l·ªói n·∫øu file b·ªã corrupt ho·∫∑c format sai
- Hi·ªÉn th·ªã t√™n file v√† s·ªë l∆∞·ª£ng samples

**V√≠ d·ª• output**:
```
‚úì Loaded: Merged01.csv - 712,311 samples
‚úì Loaded: Merged02.csv - 748,585 samples
...
```

#### 1.3. Merge t·∫•t c·∫£ DataFrame

```python
df = pd.concat(dfs, ignore_index=True)
```

**Gi·∫£i th√≠ch**:
- `pd.concat()`: G·ªôp t·∫•t c·∫£ DataFrame theo chi·ªÅu d·ªçc (vertical)
- `ignore_index=True`: T·∫°o l·∫°i index t·ª´ 0 ƒë·∫øn n-1
- K·∫øt qu·∫£: 1 DataFrame l·ªõn ch·ª©a to√†n b·ªô d·ªØ li·ªáu (~45-50 tri·ªáu samples)

**Th√¥ng tin dataset**:
```
‚Üí T·ªïng s·ªë m·∫´u: 45,000,000+ samples
‚Üí S·ªë features: 80-100 features
‚Üí K√≠ch th∆∞·ªõc: (45000000, 85) - example
```

---

### STEP 2: PH√ÇN T√çCH V√Ä TH·ªêNG K√ä DATASET

#### 2.1. X√°c ƒë·ªãnh c·ªôt Label

```python
label_col = df.columns[-1]  # C·ªôt cu·ªëi c√πng l√† label
```

**Gi·∫£i th√≠ch**:
- Dataset IoT th∆∞·ªùng c√≥ label ·ªü c·ªôt cu·ªëi
- V√≠ d·ª•: 'label', 'attack_type', 'category'

#### 2.2. Th·ªëng k√™ nh√£n g·ªëc

```python
label_counts = df[label_col].value_counts()
```

**Output m·∫´u**:
```
PH√ÇN B·ªê NH√ÉN G·ªêC:
----------------------------------------------------------------------------------------------------
T√™n nh√£n                                          S·ªë l∆∞·ª£ng         T·ª∑ l·ªá (%)
----------------------------------------------------------------------------------------------------
Benign                                              20,000,000          44.44%
DDoS                                                10,000,000          22.22%
Mirai                                                8,000,000          17.78%
DoS                                                  5,000,000          11.11%
Recon                                                2,000,000           4.44%
```

#### 2.3. Chuy·ªÉn ƒë·ªïi th√†nh Binary Labels

```python
def map_to_binary(label):
    label_lower = str(label).lower()
    if 'benign' in label_lower or 'normal' in label_lower:
        return 'Benign'  # Nh√£n 0
    else:
        return 'Attack'  # Nh√£n 1 (g·ªôp t·∫•t c·∫£ attack types)

df['binary_label'] = df[label_col].apply(map_to_binary)
```

**Gi·∫£i th√≠ch**:
- G·ªôp t·∫•t c·∫£ lo·∫°i attack th√†nh 1 class "Attack"
- ƒê∆°n gi·∫£n h√≥a b√†i to√°n t·ª´ multi-class ‚Üí binary classification
- `.apply()`: √Åp d·ª•ng h√†m cho t·ª´ng d√≤ng trong DataFrame

**Output**:
```
PH√ÇN B·ªê SAU KHI G·ªòP:
----------------------------------------------------------------------------------------------------
Nh√£n               S·ªë l∆∞·ª£ng         T·ª∑ l·ªá (%)
----------------------------------------------------------------------------------------------------
Attack              25,000,000          55.56%
Benign              20,000,000          44.44%

‚Üí T·ªâ l·ªá m·∫•t c√¢n b·∫±ng (Imbalance Ratio): 1.25:1
```

#### 2.4. Visualization - T·∫°o bi·ªÉu ƒë·ªì

```python
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Pie chart - Bi·ªÉu ƒë·ªì tr√≤n
axes[0].pie(binary_counts.values, labels=binary_counts.index, 
            autopct='%1.2f%%', colors=['#2ecc71', '#e74c3c'])

# Bar chart - Bi·ªÉu ƒë·ªì c·ªôt
axes[1].bar(binary_counts.index, binary_counts.values, 
            color=['#2ecc71', '#e74c3c'])
```

**Gi·∫£i th√≠ch**:
- `subplots(1, 2)`: T·∫°o 2 bi·ªÉu ƒë·ªì c·∫°nh nhau (1 h√†ng, 2 c·ªôt)
- Pie chart: Hi·ªÉn th·ªã t·ª∑ l·ªá ph·∫ßn trƒÉm
- Bar chart: So s√°nh s·ªë l∆∞·ª£ng tr·ª±c quan
- M√†u xanh (#2ecc71): Benign
- M√†u ƒë·ªè (#e74c3c): Attack

**Output**: File `label_distribution.png` ƒë∆∞·ª£c l∆∞u

---

### STEP 3: TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU

#### 3.1. T√°ch Features v√† Labels

```python
X = df.drop([label_col, 'binary_label'], axis=1)  # Features
y = df['binary_label']                             # Labels
```

**Gi·∫£i th√≠ch**:
- `X`: Ma tr·∫≠n features (input cho model)
- `y`: Vector labels (output/target)
- `axis=1`: X√≥a theo c·ªôt (axis=0 l√† x√≥a theo h√†ng)

**Shape**:
```
X: (45000000, 83) - 45 tri·ªáu samples, 83 features
y: (45000000,)    - 45 tri·ªáu labels
```

#### 3.2. X·ª≠ l√Ω Missing Values (Gi√° tr·ªã thi·∫øu)

```python
print(f"‚Üí Missing values: {X.isnull().sum().sum()}")
if X.isnull().sum().sum() > 0:
    X = X.fillna(0)  # Thay th·∫ø b·∫±ng 0
```

**Gi·∫£i th√≠ch**:
- `.isnull()`: Ki·ªÉm tra t·ª´ng cell c√≥ NULL kh√¥ng
- `.sum().sum()`: T·ªïng s·ªë cell NULL trong to√†n b·ªô DataFrame
- `.fillna(0)`: Thay th·∫ø NULL b·∫±ng 0

**T·∫°i sao thay b·∫±ng 0?**
- Trong network traffic, NULL th∆∞·ªùng nghƒ©a l√† "kh√¥ng c√≥ traffic"
- 0 l√† gi√° tr·ªã an to√†n, kh√¥ng l√†m sai l·ªách th·ªëng k√™
- Alternative: C√≥ th·ªÉ d√πng median, mean t√πy t·ª´ng feature

#### 3.3. X·ª≠ l√Ω Infinite Values (Gi√° tr·ªã v√¥ c·ª±c)

```python
print(f"‚Üí Infinite values: {np.isinf(X.values).sum()}")
if np.isinf(X.values).sum() > 0:
    X = X.replace([np.inf, -np.inf], 0)
```

**Gi·∫£i th√≠ch**:
- `np.inf`: V√¥ c·ª±c d∆∞∆°ng (+‚àû)
- `-np.inf`: V√¥ c·ª±c √¢m (-‚àû)
- X·∫£y ra khi chia cho 0: a/0 = ‚àû

**V√≠ d·ª•**:
```python
# Feature: packets_per_second
# packets=1000, time=0 ‚Üí packets/time = 1000/0 = inf
```

#### 3.4. Chuy·ªÉn t·∫•t c·∫£ v·ªÅ Numeric

```python
for col in X.columns:
    X[col] = pd.to_numeric(X[col], errors='coerce')
X = X.fillna(0)
```

**Gi·∫£i th√≠ch**:
- M·ªôt s·ªë c·ªôt c√≥ th·ªÉ c√≥ ki·ªÉu string, mixed types
- `pd.to_numeric()`: Chuy·ªÉn sang s·ªë
- `errors='coerce'`: N·∫øu kh√¥ng convert ƒë∆∞·ª£c ‚Üí NaN
- Sau ƒë√≥ fill NaN b·∫±ng 0

#### 3.5. Lo·∫°i b·ªè Constant Columns

```python
constant_cols = [col for col in X.columns if X[col].nunique() <= 1]
if constant_cols:
    X = X.drop(constant_cols, axis=1)
```

**Gi·∫£i th√≠ch**:
- Constant column: C·ªôt c√≥ t·∫•t c·∫£ gi√° tr·ªã gi·ªëng nhau
- V√≠ d·ª•: [0, 0, 0, ..., 0] ho·∫∑c [5, 5, 5, ..., 5]
- Kh√¥ng mang th√¥ng tin ‚Üí Lo·∫°i b·ªè ƒë·ªÉ gi·∫£m chi·ªÅu d·ªØ li·ªáu

**T·∫°i sao lo·∫°i b·ªè?**
- Kh√¥ng gi√∫p model h·ªçc ƒë∆∞·ª£c g√¨
- T·ªën memory v√† computation
- C√≥ th·ªÉ g√¢y l·ªói trong m·ªôt s·ªë algorithms

#### 3.6. Encode Labels

```python
le = LabelEncoder()
y_encoded = le.fit_transform(y)
```

**Gi·∫£i th√≠ch**:
- Chuy·ªÉn t·ª´ text ‚Üí s·ªë
- 'Benign' ‚Üí 0
- 'Attack' ‚Üí 1

**Label mapping**:
```python
{'Attack': 0, 'Benign': 1}  # ho·∫∑c ng∆∞·ª£c l·∫°i
```

---

### STEP 4: CHIA D·ªÆ LI·ªÜU

#### 4.1. Train / Validation / Test Split

```python
TEST_SIZE = 0.2     # 20% cho test
VAL_SIZE = 0.125    # ~10% cho validation

# Chia train+val v√† test
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X, y_encoded, 
    test_size=TEST_SIZE,      # 20%
    random_state=42, 
    stratify=y_encoded        # Gi·ªØ t·ª∑ l·ªá class
)

# Chia train v√† validation
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val,
    test_size=VAL_SIZE,       # 10% of total
    random_state=42,
    stratify=y_train_val
)
```

**Gi·∫£i th√≠ch stratify**:
- Gi·ªØ nguy√™n t·ª∑ l·ªá gi·ªØa c√°c class trong m·ªói t·∫≠p
- V√≠ d·ª•: N·∫øu dataset c√≥ 60% Attack, 40% Benign
  - Train set c≈©ng s·∫Ω c√≥ ~60% Attack, ~40% Benign
  - Test set c≈©ng s·∫Ω c√≥ ~60% Attack, ~40% Benign

**T·ª∑ l·ªá cu·ªëi c√πng**:
```
Training:   70% (~31,500,000 samples)
Validation: 10% (~4,500,000 samples)
Test:       20% (~9,000,000 samples)
```

**T·∫°i sao chia nh∆∞ v·∫≠y?**
- **Train set**: L·ªõn nh·∫•t, ƒë·ªÉ model h·ªçc
- **Validation set**: Tune hyperparameters, early stopping
- **Test set**: ƒê√°nh gi√° cu·ªëi c√πng (model ch∆∞a t·ª´ng th·∫•y)

---

### STEP 5: CHU·∫®N H√ìA D·ªÆ LI·ªÜU (Normalization)

#### 5.1. StandardScaler

```python
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)
```

**C√¥ng th·ª©c StandardScaler**:
```
z = (x - Œº) / œÉ

Trong ƒë√≥:
- x: Gi√° tr·ªã g·ªëc
- Œº (mu): Mean (trung b√¨nh)
- œÉ (sigma): Standard deviation (ƒë·ªô l·ªách chu·∫©n)
- z: Gi√° tr·ªã ƒë√£ chu·∫©n h√≥a
```

**V√≠ d·ª• c·ª• th·ªÉ**:
```python
# Feature: packet_size
# Values: [64, 128, 256, 512, 1024]
# Mean (Œº) = 396.8
# Std (œÉ) = 369.4

# Chu·∫©n h√≥a:
# 64   ‚Üí (64 - 396.8) / 369.4 = -0.90
# 128  ‚Üí (128 - 396.8) / 369.4 = -0.73
# 256  ‚Üí (256 - 396.8) / 369.4 = -0.38
# 512  ‚Üí (512 - 396.8) / 369.4 = 0.31
# 1024 ‚Üí (1024 - 396.8) / 369.4 = 1.70
```

**T·∫°i sao c·∫ßn chu·∫©n h√≥a?**
1. **C√°c features c√≥ scale kh√°c nhau**:
   - packet_size: 0-65535
   - packet_count: 0-1000000
   - duration: 0-3600
2. **Neural Network ho·∫°t ƒë·ªông t·ªët h∆°n** v·ªõi d·ªØ li·ªáu chu·∫©n h√≥a
3. **Tr√°nh features c√≥ gi√° tr·ªã l·ªõn** dominate qu√° tr√¨nh training
4. **Gi√∫p gradient descent h·ªôi t·ª• nhanh h∆°n**

**L∆∞u √Ω quan tr·ªçng**:
- `fit_transform()` ch·ªâ d√πng cho **train set**
- `transform()` cho val v√† test (d√πng Œº v√† œÉ t·ª´ train set)
- **KH√îNG fit tr√™n val/test** ƒë·ªÉ tr√°nh data leakage!

---

## üß† X√ÇY D·ª∞NG M√î H√åNH CNN-GRU

### STEP 6: KI·∫æN TR√öC MODEL

#### 6.1. T·ªïng quan ki·∫øn tr√∫c

```
Input (83 features)
    |
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    |                 |                 |
CNN Module        GRU Module      (Parallel)
    |                 |
Conv Block 1      GRU Layer 1
Conv Block 2      GRU Layer 2
Conv Block 3          |
    |                 |
  Flatten             |
    |                 |
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                      |
                 Concatenate
                      |
                 MLP Module
                      |
                Dense Layer 1 (256 units)
                Dense Layer 2 (128 units)
                      |
                Output Layer (2 classes)
                      |
                  Softmax
```

#### 6.2. Input Layer

```python
input_layer = layers.Input(shape=input_shape, name='input')
# input_shape = (83,) - 83 features
```

**Gi·∫£i th√≠ch**:
- Nh·∫≠n input vector 83 chi·ªÅu
- Shape: (batch_size, 83)

#### 6.3. CNN Module - Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng kh√¥ng gian

**Conv Block 1**:
```python
x_cnn = layers.Reshape((input_shape[0], 1), name='reshape_cnn')(input_layer)
# (batch, 83) ‚Üí (batch, 83, 1)

x_cnn = layers.Conv1D(filters=64, kernel_size=3, padding='same', 
                      activation='relu', name='conv1')(x_cnn)
# Output: (batch, 83, 64)

x_cnn = layers.BatchNormalization(name='bn1')(x_cnn)
# Chu·∫©n h√≥a output c·ªßa conv layer

x_cnn = layers.MaxPooling1D(pool_size=2, name='pool1')(x_cnn)
# Output: (batch, 41, 64) - gi·∫£m m·ªôt n·ª≠a
```

**Gi·∫£i th√≠ch t·ª´ng layer**:

1. **Reshape**: 
   - Chuy·ªÉn t·ª´ vector (83,) ‚Üí matrix (83, 1)
   - C·∫ßn thi·∫øt cho Conv1D

2. **Conv1D**:
   - `filters=64`: T·∫°o 64 feature maps
   - `kernel_size=3`: C·ª≠a s·ªï tr∆∞·ª£t size 3
   - `padding='same'`: Gi·ªØ nguy√™n k√≠ch th∆∞·ªõc
   - `activation='relu'`: H√†m k√≠ch ho·∫°t ReLU

3. **BatchNormalization**:
   - Chu·∫©n h√≥a output
   - TƒÉng t·ªëc training, tr√°nh overfitting

4. **MaxPooling1D**:
   - Gi·∫£m k√≠ch th∆∞·ªõc xu·ªëng 1/2
   - Gi·ªØ l·∫°i gi√° tr·ªã max trong m·ªói window
   - Gi·∫£m computation, tƒÉng receptive field

**Conv Block 2 & 3**: T∆∞∆°ng t·ª± nh∆∞ng tƒÉng s·ªë filters (128, 256)

**Flatten**:
```python
cnn_output = layers.Flatten(name='flatten_cnn')(x_cnn)
# Output: (batch, features_dim)
```

#### 6.4. GRU Module - H·ªçc m·∫´u tu·∫ßn t·ª±

```python
x_gru = layers.Reshape((input_shape[0], 1), name='reshape_gru')(input_layer)
# (batch, 83, 1)

x_gru = layers.GRU(units=128, return_sequences=True, name='gru1')(x_gru)
# Output: (batch, 83, 128)

x_gru = layers.GRU(units=64, return_sequences=False, name='gru2')(x_gru)
# Output: (batch, 64)
```

**Gi·∫£i th√≠ch GRU**:
- **GRU (Gated Recurrent Unit)**: Bi·∫øn th·ªÉ c·ªßa LSTM, ƒë∆°n gi·∫£n h∆°n
- **return_sequences=True**: Tr·∫£ v·ªÅ output cho m·ªçi timestep
- **return_sequences=False**: Ch·ªâ tr·∫£ v·ªÅ output cu·ªëi c√πng

**T·∫°i sao d√πng GRU?**
- H·ªçc ƒë∆∞·ª£c temporal dependencies (ph·ª• thu·ªôc th·ªùi gian)
- Trong network traffic, c√≥ th·ªÉ c√≥ pattern theo th·ªùi gian
- V√≠ d·ª•: T·ªëc ƒë·ªô tƒÉng d·∫ßn, burst traffic, v.v.

#### 6.5. Concatenate - K·∫øt h·ª£p CNN v√† GRU

```python
concatenated = layers.Concatenate(name='concatenate')([cnn_output, gru_output])
```

**Gi·∫£i th√≠ch**:
- Gh√©p output c·ªßa CNN v√† GRU theo chi·ªÅu features
- CNN: Spatial features
- GRU: Temporal features
- Combined: C·∫£ hai lo·∫°i features

#### 6.6. MLP Module - Ph√¢n lo·∫°i

```python
# Dense Layer 1
x = layers.Dense(256, activation='relu', name='dense1')(concatenated)
x = layers.BatchNormalization(name='bn_mlp1')(x)
x = layers.Dropout(0.5, name='dropout1')(x)
# Dropout 50%: Randomly t·∫Øt 50% neurons

# Dense Layer 2
x = layers.Dense(128, activation='relu', name='dense2')(x)
x = layers.BatchNormalization(name='bn_mlp2')(x)
x = layers.Dropout(0.3, name='dropout2')(x)
# Dropout 30%
```

**Gi·∫£i th√≠ch Dropout**:
- Randomly "t·∫Øt" m·ªôt s·ªë neurons trong training
- Tr√°nh overfitting (model h·ªçc qu√° k·ªπ training data)
- 0.5 = t·∫Øt 50%, 0.3 = t·∫Øt 30%
- Ch·ªâ ho·∫°t ƒë·ªông trong training, kh√¥ng d√πng trong inference

#### 6.7. Output Layer

```python
output = layers.Dense(num_classes, activation='softmax', name='output')(x)
# num_classes = 2 (Benign, Attack)
```

**Softmax activation**:
```
Softmax(x_i) = exp(x_i) / Œ£ exp(x_j)

Output: [0.3, 0.7]
        ‚Üì     ‚Üì
     Benign Attack
```

**Gi·∫£i th√≠ch**:
- Chuy·ªÉn logits th√†nh x√°c su·∫•t
- T·ªïng c√°c x√°c su·∫•t = 1
- V√≠ d·ª•: [0.3, 0.7] ‚Üí 30% Benign, 70% Attack

#### 6.8. Compile Model

```python
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy', 
             keras.metrics.Precision(name='precision'),
             keras.metrics.Recall(name='recall')]
)
```

**Gi·∫£i th√≠ch c√°c th√†nh ph·∫ßn**:

1. **Optimizer: Adam**
   - Adaptive Moment Estimation
   - K·∫øt h·ª£p momentum v√† adaptive learning rate
   - learning_rate=0.001: B∆∞·ªõc nh·∫£y trong gradient descent

2. **Loss: Sparse Categorical Crossentropy**
   ```
   Loss = -Œ£ y_true * log(y_pred)
   ```
   - Sparse: Labels l√† integers (0, 1) thay v√¨ one-hot
   - ƒêo s·ª± kh√°c bi·ªát gi·ªØa d·ª± ƒëo√°n v√† ground truth

3. **Metrics**:
   - **Accuracy**: T·ª∑ l·ªá d·ª± ƒëo√°n ƒë√∫ng
   - **Precision**: Trong c√°c d·ª± ƒëo√°n Attack, bao nhi√™u % ƒë√∫ng
   - **Recall**: Trong c√°c Attack th·ª±c t·∫ø, bao nhi√™u % ƒë∆∞·ª£c ph√°t hi·ªán

---

### STEP 7: HU·∫§N LUY·ªÜN M√î H√åNH

#### 7.1. Hyperparameters

```python
EPOCHS = 50          # S·ªë l·∫ßn duy·ªát qua to√†n b·ªô dataset
BATCH_SIZE = 128     # S·ªë samples trong 1 batch
```

**Gi·∫£i th√≠ch**:
- **1 Epoch**: Model xem qua t·∫•t c·∫£ training samples 1 l·∫ßn
- **Batch**: Chia nh·ªè data th√†nh c√°c batch ƒë·ªÉ train
- **Batch size 128**: M·ªói l·∫ßn update weights, d√πng 128 samples

**T·∫°i sao kh√¥ng train to√†n b·ªô dataset c√πng l√∫c?**
- Dataset qu√° l·ªõn (45M samples) kh√¥ng fit v√†o RAM/GPU
- Mini-batch gradient descent nhanh h∆°n v√† ·ªïn ƒë·ªãnh h∆°n

#### 7.2. Callbacks

**EarlyStopping**:
```python
EarlyStopping(
    monitor='val_loss',        # Theo d√µi validation loss
    patience=10,               # Ch·ªù 10 epochs
    restore_best_weights=True  # Kh√¥i ph·ª•c weights t·ªët nh·∫•t
)
```

**Gi·∫£i th√≠ch**:
- N·∫øu val_loss kh√¥ng gi·∫£m sau 10 epochs ‚Üí D·ª´ng training
- Tr√°nh l√£ng ph√≠ th·ªùi gian khi model ƒë√£ converge
- Restore best weights: D√πng model t·ªët nh·∫•t, kh√¥ng ph·∫£i model cu·ªëi

**ReduceLROnPlateau**:
```python
ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,                # Gi·∫£m learning rate xu·ªëng 50%
    patience=5,
    min_lr=1e-7
)
```

**Gi·∫£i th√≠ch**:
- N·∫øu val_loss kh√¥ng gi·∫£m sau 5 epochs ‚Üí Gi·∫£m learning rate
- lr_new = lr_old * 0.5
- Gi√∫p model t√¨m ƒë∆∞·ª£c minimum t·ªët h∆°n

**ModelCheckpoint**:
```python
ModelCheckpoint(
    'best_model.h5',
    monitor='val_accuracy',    # Theo d√µi val accuracy
    save_best_only=True        # Ch·ªâ l∆∞u model t·ªët nh·∫•t
)
```

**Gi·∫£i th√≠ch**:
- T·ª± ƒë·ªông l∆∞u model t·ªët nh·∫•t trong qu√° tr√¨nh training
- Kh√¥ng c·∫ßn ph·∫£i train l·∫°i n·∫øu mu·ªën d√πng best model

#### 7.3. Training Process

```python
history = model.fit(
    X_train_scaled, y_train,
    validation_data=(X_val_scaled, y_val),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    callbacks=callbacks,
    verbose=1
)
```

**Qu√° tr√¨nh training m·ªói epoch**:
```
1. Forward pass: T√≠nh output t·ª´ input
2. Compute loss: So s√°nh output vs ground truth
3. Backward pass: T√≠nh gradient
4. Update weights: weights -= lr * gradient
5. Evaluate tr√™n validation set
6. Check callbacks (early stopping, reduce LR, etc.)
```

**Output m·∫´u**:
```
Epoch 1/50
246094/246094 [==============================] - 450s 2ms/step
loss: 0.2534 - accuracy: 0.9245 - precision: 0.9156 - recall: 0.9345
val_loss: 0.2145 - val_accuracy: 0.9367 - val_precision: 0.9287 - val_recall: 0.9445
Epoch 00001: val_accuracy improved from -inf to 0.93674, saving model to best_model.h5

Epoch 2/50
...
```

---

### STEP 8: VISUALIZE TRAINING HISTORY

#### 8.1. Plot Loss & Metrics

```python
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# Loss curve
axes[0, 0].plot(history.history['loss'], label='Train Loss')
axes[0, 0].plot(history.history['val_loss'], label='Val Loss')
```

**Gi·∫£i th√≠ch c√°c bi·ªÉu ƒë·ªì**:

1. **Loss Curve**:
   - Train loss gi·∫£m: Model ƒëang h·ªçc
   - Val loss gi·∫£m: Model generalize t·ªët
   - Val loss tƒÉng: C√≥ th·ªÉ b·ªã overfitting

2. **Accuracy Curve**:
   - Th·ªÉ hi·ªán % d·ª± ƒëo√°n ƒë√∫ng qua m·ªói epoch
   - Train acc th∆∞·ªùng cao h∆°n val acc

3. **Precision Curve**:
   - Precision cao: √çt False Positive
   - Quan tr·ªçng khi cost of FP cao

4. **Recall Curve**:
   - Recall cao: √çt False Negative
   - Quan tr·ªçng trong intrusion detection (ph·∫£i ph√°t hi·ªán ƒë∆∞·ª£c attack!)

**V√≠ d·ª• ƒë·ªçc bi·ªÉu ƒë·ªì**:
```
Epoch 1:  train_loss=0.50, val_loss=0.45 ‚úì Good
Epoch 10: train_loss=0.20, val_loss=0.22 ‚úì Good
Epoch 20: train_loss=0.10, val_loss=0.25 ‚ö† Overfitting warning
Epoch 30: train_loss=0.05, val_loss=0.30 ‚úó Overfitting!
```

---

### STEP 9: ƒê√ÅNH GI√Å M√î H√åNH

#### 9.1. Predictions

```python
y_pred_proba = model.predict(X_test_scaled, verbose=0)
# Output: [[0.3, 0.7], [0.9, 0.1], ...]
#          x√°c su·∫•t cho m·ªói class

y_pred = np.argmax(y_pred_proba, axis=1)
# L·∫•y class c√≥ x√°c su·∫•t cao nh·∫•t
# [1, 0, 1, 0, ...]
```

#### 9.2. Metrics

**Accuracy**:
```python
accuracy = accuracy_score(y_test, y_pred)
# Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

**Precision**:
```python
precision = precision_score(y_test, y_pred, average='binary')
# Precision = TP / (TP + FP)
# "Trong c√°c d·ª± ƒëo√°n Attack, bao nhi√™u % th·∫≠t s·ª± l√† Attack?"
```

**Recall**:
```python
recall = recall_score(y_test, y_pred, average='binary')
# Recall = TP / (TP + FN)
# "Trong c√°c Attack th·ª±c t·∫ø, bao nhi√™u % ƒë∆∞·ª£c ph√°t hi·ªán?"
```

**F1-Score**:
```python
f1 = f1_score(y_test, y_pred, average='binary')
# F1 = 2 * (Precision * Recall) / (Precision + Recall)
# Trung b√¨nh ƒëi·ªÅu h√≤a c·ªßa Precision v√† Recall
```

#### 9.3. Confusion Matrix

```
                Predicted
                Benign  Attack
Actual Benign     TN      FP
       Attack     FN      TP
```

**V√≠ d·ª• c·ª• th·ªÉ**:
```
                Predicted
                Benign    Attack
Actual Benign   3,800,000  200,000  (FP: False Alarm)
       Attack     300,000 4,700,000  (FN: Missed Attack)

TN = 3,800,000: D·ª± ƒëo√°n ƒë√∫ng Benign
FP =   200,000: D·ª± ƒëo√°n nh·∫ßm l√† Attack (False Alarm)
FN =   300,000: D·ª± ƒëo√°n nh·∫ßm l√† Benign (Missed Attack) ‚ö†
TP = 4,700,000: D·ª± ƒëo√°n ƒë√∫ng Attack
```

**Ph√¢n t√≠ch**:
- **FP (False Positive)**: Benign b·ªã nh·∫ßm l√† Attack
  - Consequence: False alarm, block traffic b√¨nh th∆∞·ªùng
  - √çt nghi√™m tr·ªçng h∆°n FN

- **FN (False Negative)**: Attack b·ªã nh·∫ßm l√† Benign
  - Consequence: Attack kh√¥ng b·ªã ph√°t hi·ªán!
  - R·∫•t nghi√™m tr·ªçng trong security!

**Trade-off**:
- Precision cao ‚Üí FP th·∫•p ‚Üí √çt false alarm
- Recall cao ‚Üí FN th·∫•p ‚Üí √çt missed attack
- Th∆∞·ªùng ph·∫£i balance gi·ªØa hai metrics n√†y

#### 9.4. Classification Report

```
              precision    recall  f1-score   support

      Benign     0.9267    0.9500    0.9382   4000000
      Attack     0.9592    0.9400    0.9495   5000000

    accuracy                         0.9444   9000000
   macro avg     0.9430    0.9450    0.9439   9000000
weighted avg     0.9447    0.9444    0.9445   9000000
```

**Gi·∫£i th√≠ch**:
- **support**: S·ªë samples th·ª±c t·∫ø c·ªßa class ƒë√≥
- **macro avg**: Trung b√¨nh ƒë∆°n gi·∫£n c·ªßa 2 classes
- **weighted avg**: Trung b√¨nh c√≥ tr·ªçng s·ªë (theo support)

---

### STEP 10: L∆ØU K·∫æT QU·∫¢

#### 10.1. L∆∞u Model

```python
model.save('final_cnn_gru_model.h5')
```

**Gi·∫£i th√≠ch**:
- L∆∞u to√†n b·ªô model: architecture + weights + optimizer state
- Format: HDF5 (.h5)
- C√≥ th·ªÉ load l·∫°i ƒë·ªÉ d√πng: `model = keras.models.load_model('final_cnn_gru_model.h5')`

#### 10.2. L∆∞u Scaler v√† Label Encoder

```python
import joblib
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(le, 'label_encoder.pkl')
```

**T·∫°i sao ph·∫£i l∆∞u?**
- Khi d√πng model ƒë·ªÉ predict d·ªØ li·ªáu m·ªõi:
  1. Ph·∫£i chu·∫©n h√≥a d·ªØ li·ªáu v·ªõi **c√πng scaler**
  2. Ph·∫£i decode labels v·ªõi **c√πng label encoder**

**V√≠ d·ª• s·ª≠ d·ª•ng**:
```python
# Load model v√† scaler
model = keras.models.load_model('final_cnn_gru_model.h5')
scaler = joblib.load('scaler.pkl')
le = joblib.load('label_encoder.pkl')

# Predict d·ªØ li·ªáu m·ªõi
X_new = pd.read_csv('new_traffic.csv')
X_new_scaled = scaler.transform(X_new)  # D√πng scaler ƒë√£ fit
predictions = model.predict(X_new_scaled)
labels = le.inverse_transform(predictions.argmax(axis=1))
print(labels)  # ['Benign', 'Attack', 'Attack', ...]
```

---

## üìä ƒê√ÅNH GI√Å V√Ä PH√ÇN T√çCH

### Metrics Quan Tr·ªçng trong Intrusion Detection

#### 1. Recall (Sensitivity)
**Quan tr·ªçng nh·∫•t!**
- Ph·∫£i ph√°t hi·ªán ƒë∆∞·ª£c c√†ng nhi·ªÅu attack c√†ng t·ªët
- Recall th·∫•p ‚Üí Nhi·ªÅu attack b·ªã b·ªè s√≥t ‚Üí Nguy hi·ªÉm!

#### 2. Precision
- Quan tr·ªçng ƒë·ªÉ tr√°nh false alarm
- Precision th·∫•p ‚Üí Nhi·ªÅu traffic b√¨nh th∆∞·ªùng b·ªã block ‚Üí User experience k√©m

#### 3. F1-Score
- Balance gi·ªØa Precision v√† Recall
- Th∆∞·ªùng d√πng ƒë·ªÉ so s√°nh c√°c models

#### 4. Accuracy
- **C·∫©n th·∫≠n v·ªõi imbalanced dataset!**
- V√≠ d·ª•: 95% Benign, 5% Attack
  - Model d·ª± ƒëo√°n t·∫•t c·∫£ l√† Benign ‚Üí Accuracy = 95%
  - Nh∆∞ng Recall = 0% ‚Üí V√¥ d·ª•ng!

### So s√°nh v·ªõi Baseline

**Baseline models th∆∞·ªùng d√πng**:
- Logistic Regression
- Random Forest
- SVM
- Simple Neural Network

**M·ª•c ti√™u**:
- CNN-GRU ph·∫£i t·ªët h∆°n baseline √≠t nh·∫•t 2-5%
- Trade-off gi·ªØa performance v√† complexity

---

## üöÄ TIPS V√Ä TRICKS

### 1. TƒÉng Performance

**TƒÉng Recall** (ph√°t hi·ªán nhi·ªÅu attack h∆°n):
- Gi·∫£m threshold c·ªßa classification
  ```python
  threshold = 0.3  # instead of 0.5
  y_pred = (y_pred_proba[:, 1] > threshold).astype(int)
  ```
- TƒÉng class weight cho Attack
  ```python
  class_weight = {0: 1.0, 1: 2.0}  # Attack c√≥ weight g·∫•p ƒë√¥i
  model.fit(..., class_weight=class_weight)
  ```

**Gi·∫£m Overfitting**:
- TƒÉng Dropout rate (0.5 ‚Üí 0.6, 0.7)
- Th√™m L2 regularization
  ```python
  layers.Dense(256, kernel_regularizer=keras.regularizers.l2(0.01))
  ```
- Data augmentation
- Early stopping v·ªõi patience nh·ªè h∆°n

**TƒÉng Speed**:
- Gi·∫£m batch size (nh∆∞ng c√≥ th·ªÉ gi·∫£m performance)
- D√πng GPU (CUDA)
- Mixed precision training (FP16)
- Model pruning/quantization

### 2. Debug Common Issues

**Problem: Loss kh√¥ng gi·∫£m**
- Check learning rate (qu√° cao ho·∫∑c qu√° th·∫•p)
- Check data preprocessing (c√≥ normalize ch∆∞a?)
- Check label encoding (ƒë√∫ng format ch∆∞a?)

**Problem: Overfitting**
- TƒÉng Dropout
- Th√™m regularization
- Gi·∫£m model complexity
- TƒÉng training data

**Problem: Underfitting**
- TƒÉng model complexity (th√™m layers, units)
- Gi·∫£m regularization
- Train l√¢u h∆°n
- Check data quality

### 3. Hyperparameter Tuning

**Learning Rate**:
```python
# Try: 0.1, 0.01, 0.001, 0.0001
lr_options = [1e-2, 1e-3, 1e-4]
```

**Batch Size**:
```python
# Try: 32, 64, 128, 256, 512
batch_options = [64, 128, 256]
```

**Architecture**:
```python
# Try different:
# - Number of Conv blocks
# - Number of filters
# - GRU units
# - Dense layer sizes
```

---

## üìù K·∫æT LU·∫¨N

### ∆Øu ƒëi·ªÉm c·ªßa CNN-GRU

1. **K·∫øt h·ª£p spatial v√† temporal features**
   - CNN: Local patterns trong features
   - GRU: Sequential patterns

2. **Performance cao**
   - Th∆∞·ªùng ƒë·∫°t 95-99% accuracy
   - Recall cao ‚Üí Ph√°t hi·ªán attack t·ªët

3. **Robust**
   - Handle ƒë∆∞·ª£c imbalanced data
   - Generalize t·ªët

### Nh∆∞·ª£c ƒëi·ªÉm

1. **Computational cost**
   - Training l√¢u (v√†i gi·ªù v·ªõi dataset l·ªõn)
   - C·∫ßn GPU ƒë·ªÉ train hi·ªáu qu·∫£

2. **Complex architecture**
   - Kh√≥ debug
   - Nhi·ªÅu hyperparameters c·∫ßn tune

3. **Black box**
   - Kh√≥ gi·∫£i th√≠ch t·∫°i sao model d·ª± ƒëo√°n nh∆∞ v·∫≠y
   - C·∫ßn th√™m explainability techniques

### H∆∞·ªõng ph√°t tri·ªÉn

1. **Attention Mechanism**
   - Th√™m attention layer ƒë·ªÉ focus v√†o important features

2. **Ensemble Learning**
   - K·∫øt h·ª£p nhi·ªÅu models
   - Voting ho·∫∑c stacking

3. **Real-time Detection**
   - Optimize cho inference speed
   - Deploy v·ªõi TensorFlow Lite, ONNX

4. **Explainability**
   - SHAP, LIME ƒë·ªÉ gi·∫£i th√≠ch predictions
   - Feature importance analysis

---

## üìö T√ÄI LI·ªÜU THAM KH·∫¢O

### Papers
- DeepFed Paper: Federated Learning Architecture
- CNN for Network Traffic Classification
- GRU vs LSTM comparison

### Libraries Documentation
- TensorFlow/Keras: https://www.tensorflow.org/
- Scikit-learn: https://scikit-learn.org/
- Pandas: https://pandas.pydata.org/

### Courses
- Deep Learning Specialization (Coursera)
- TensorFlow Developer Certificate
- Network Security v√† Intrusion Detection

---

## üí° FAQ - C√¢u h·ªèi th∆∞·ªùng g·∫∑p

### Q1: T·∫°i sao d√πng binary classification thay v√¨ multi-class?
**A**: 
- ƒê∆°n gi·∫£n h∆°n, d·ªÖ deploy h∆°n
- Performance th∆∞·ªùng t·ªët h∆°n
- Trong th·ª±c t·∫ø, quan tr·ªçng nh·∫•t l√† ph√°t hi·ªán "c√≥ attack hay kh√¥ng"
- Chi ti·∫øt lo·∫°i attack c√≥ th·ªÉ ph√°t hi·ªán ·ªü stage 2

### Q2: Dataset qu√° l·ªõn, kh√¥ng ƒë·ªß RAM?
**A**:
- D√πng `batch_size` nh·ªè h∆°n
- D√πng generator thay v√¨ load to√†n b·ªô:
  ```python
  def data_generator(X, y, batch_size):
      while True:
          for i in range(0, len(X), batch_size):
              yield X[i:i+batch_size], y[i:i+batch_size]
  ```
- Downsample dataset (l·∫•y subset)
- D√πng cloud computing (AWS, GCP)

### Q3: Training qu√° l√¢u?
**A**:
- D√πng GPU (NVIDIA CUDA)
- Gi·∫£m s·ªë epochs
- Gi·∫£m batch size
- Simplify model architecture
- D√πng pretrained model

### Q4: L√†m sao improve Recall?
**A**:
- Adjust classification threshold
- Class weighting
- Oversample minority class (SMOTE)
- Focal loss thay v√¨ cross-entropy
- Ensemble v·ªõi nhi·ªÅu models

### Q5: Model b·ªã overfitting?
**A**:
- TƒÉng Dropout (0.5 ‚Üí 0.7)
- L2 regularization
- Early stopping v·ªõi patience nh·ªè
- Data augmentation
- Reduce model complexity

---

**Ch√∫c b·∫°n th√†nh c√¥ng v·ªõi d·ª± √°n! üéâ**

*N·∫øu c√≥ c√¢u h·ªèi, vui l√≤ng tham kh·∫£o documentation ho·∫∑c contact!*
