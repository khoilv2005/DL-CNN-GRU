import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')  # ‚Üê FIX: D√πng backend kh√¥ng c·∫ßn GUI (quan tr·ªçng cho WSL!)
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score, f1_score
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
import joblib  # ‚Üê FIX: Import ·ªü ƒë·∫ßu file thay v√¨ gi·ªØa code
import os
import warnings
import shutil
from datetime import datetime
warnings.filterwarnings('ignore')

# Set random seed
np.random.seed(42)
tf.random.set_seed(42)

# Set style
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("=" * 100)
print(" " * 30 + "CNN-GRU MODEL FOR IoT INTRUSION DETECTION")
print(" " * 35 + "Based on DeepFed Paper Architecture")
print("=" * 100)

# Check GPU availability
print("\n" + "=" * 100)
print("KI·ªÇM TRA THI·∫æT B·ªä T√çNH TO√ÅN")
print("=" * 100)

gpus = tf.config.list_physical_devices('GPU')
if gpus:
    print(f"\n‚úì T√åM TH·∫§Y {len(gpus)} GPU!")
    for i, gpu in enumerate(gpus):
        gpu_details = tf.config.experimental.get_device_details(gpu)
        gpu_name = gpu_details.get('device_name', 'Unknown GPU')
        print(f"  GPU {i}: {gpu_name}")
        print(f"  Device: {gpu.name}")
    print(f"\n‚Üí MODEL S·∫º TRAIN TR√äN GPU (nhanh h∆°n 5-10 l·∫ßn)")
else:
    print("\n‚úó KH√îNG T√åM TH·∫§Y GPU")
    print("‚Üí Model s·∫Ω train tr√™n CPU (ch·∫≠m h∆°n)")

print("=" * 100)

# ================================================================================
# 0. CREATE BACKUP FOLDER
# ================================================================================

# T·∫°o th∆∞ m·ª•c backup v·ªõi timestamp
TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M%S")
BACKUP_FOLDER = f"./backup_{TIMESTAMP}"
os.makedirs(BACKUP_FOLDER, exist_ok=True)

print("\n" + "=" * 100)
print("BACKUP FOLDER")
print("=" * 100)
print(f"\n‚úì T·∫°o th∆∞ m·ª•c backup: {BACKUP_FOLDER}")
print(f"  ‚Üí T·∫•t c·∫£ k·∫øt qu·∫£ s·∫Ω ƒë∆∞·ª£c l∆∞u t·∫°i ƒë√¢y")
print("=" * 100)

# ================================================================================
# 1. LOAD AND ANALYZE DATASET
# ================================================================================

print("\n" + "=" * 100)
print("STEP 1: LOAD AND ANALYZE DATASET")
print("=" * 100)

DATA_PATH = './IoT_Dataset_2023'  # Thay ƒë·ªïi path c·ªßa b·∫°n

# T√¨m t·∫•t c·∫£ file CSV
csv_files = []
for root, dirs, files in os.walk(DATA_PATH):
    for file in files:
        if file.endswith('.csv'):
            csv_files.append(os.path.join(root, file))

# S·ª≠ d·ª•ng 20 FILES ƒë·ªÉ c√≥ ƒë·ªß data (theo paper DeepFed c·∫ßn dataset l·ªõn)
csv_files = sorted(csv_files)[:20]  # L·∫•y 20 files (~14-15M samples)

print(f"\nT√¨m th·∫•y {len(csv_files)} file CSV (s·ª≠ d·ª•ng 20 files ƒë·ªÉ c√≥ ƒë·ªß data)")
print("-" * 100)

# Load t·∫•t c·∫£ files
dfs = []
total_loaded = 0
for file in csv_files:
    try:
        df_temp = pd.read_csv(file)
        dfs.append(df_temp)
        total_loaded += 1
        print(f"‚úì Loaded: {os.path.basename(file):50s} - {len(df_temp):>10,} samples")
    except MemoryError:
        print(f"\n‚ö†Ô∏è  C·∫¢NH B√ÅO: H·∫æT RAM khi load file {os.path.basename(file)}")
        print(f"‚Üí ƒê√£ load ƒë∆∞·ª£c {total_loaded}/{len(csv_files)} files. Ti·∫øp t·ª•c v·ªõi {total_loaded} files...")
        break
    except Exception as e:
        print(f"‚úó Error loading {os.path.basename(file)}: {e}")

if len(dfs) == 0:
    raise ValueError("‚ùå KH√îNG TH·ªÇ LOAD B·∫§T K·ª≤ FILE N√ÄO! Ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n ho·∫∑c RAM.")

# Merge t·∫•t c·∫£
print(f"\n‚Üí ƒêang merge {len(dfs)} files...")
df = pd.concat(dfs, ignore_index=True)
del dfs  # ‚Üê FIX: Gi·∫£i ph√≥ng RAM ngay sau khi merge

print("\n" + "-" * 100)
print(f"‚Üí T·ªïng s·ªë m·∫´u: {len(df):,}")
print(f"‚Üí S·ªë features: {len(df.columns)}")
print(f"‚Üí K√≠ch th∆∞·ªõc dataset: {df.shape}")

# ================================================================================
# 2. DATA ANALYSIS AND STATISTICS
# ================================================================================

print("\n" + "=" * 100)
print("STEP 2: PH√ÇN T√çCH V√Ä TH·ªêNG K√ä DATASET")
print("=" * 100)

# T√¨m c·ªôt label (th∆∞·ªùng l√† c·ªôt cu·ªëi)
label_col = df.columns[-1]
print(f"\nC·ªôt nh√£n: {label_col}")

# Th·ªëng k√™ nh√£n g·ªëc
print("\n" + "-" * 100)
print("PH√ÇN B·ªê NH√ÉN G·ªêC:")
print("-" * 100)
label_counts = df[label_col].value_counts()
print(f"\n{'T√™n nh√£n':<50s} {'S·ªë l∆∞·ª£ng':>15s} {'T·ª∑ l·ªá (%)':>10s}")
print("-" * 100)
for label, count in label_counts.items():
    percentage = (count / len(df)) * 100
    print(f"{str(label):<50s} {count:>15,} {percentage:>10.2f}%")

# Chuy·ªÉn ƒë·ªïi th√†nh Binary labels
print("\n" + "-" * 100)
print("CHUY·ªÇN ƒê·ªîI TH√ÄNH 2 L·ªöP: BENIGN vs ATTACK")
print("-" * 100)

def map_to_binary(label):
    label_lower = str(label).lower()
    if 'benign' in label_lower or 'normal' in label_lower:
        return 'Benign'
    else:
        return 'Attack'

df['binary_label'] = df[label_col].apply(map_to_binary)

# Th·ªëng k√™ Binary labels
print("\nPH√ÇN B·ªê SAU KHI G·ªòP:")
print("-" * 100)
binary_counts = df['binary_label'].value_counts()
print(f"\n{'Nh√£n':<15s} {'S·ªë l∆∞·ª£ng':>15s} {'T·ª∑ l·ªá (%)':>10s}")
print("-" * 100)
for label, count in binary_counts.items():
    percentage = (count / len(df)) * 100
    print(f"{label:<15s} {count:>15,} {percentage:>10.2f}%")

# T√≠nh t·ªâ l·ªá m·∫•t c√¢n b·∫±ng
imbalance_ratio = binary_counts.max() / binary_counts.min()
print(f"\n‚Üí T·ªâ l·ªá m·∫•t c√¢n b·∫±ng (Imbalance Ratio): {imbalance_ratio:.2f}:1")

# Visualization
print("\n" + "-" * 100)
print("T·∫†O BI·ªÇU ƒê·ªí PH√ÇN B·ªê NH√ÉN")
print("-" * 100)

fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Pie chart
colors = ['#2ecc71', '#e74c3c']
explode = (0.05, 0)
axes[0].pie(binary_counts.values, labels=binary_counts.index, autopct='%1.2f%%',
           colors=colors, startangle=90, explode=explode, shadow=True)
axes[0].set_title('Ph√¢n b·ªë nh√£n - Pie Chart', fontsize=16, fontweight='bold', pad=20)

# Bar chart
bars = axes[1].bar(binary_counts.index, binary_counts.values, color=colors, 
                   alpha=0.8, edgecolor='black', linewidth=1.5)
axes[1].set_xlabel('Label', fontsize=14, fontweight='bold')
axes[1].set_ylabel('S·ªë l∆∞·ª£ng m·∫´u', fontsize=14, fontweight='bold')
axes[1].set_title('Ph√¢n b·ªë nh√£n - Bar Chart', fontsize=16, fontweight='bold', pad=20)
axes[1].grid(axis='y', alpha=0.3)

# Th√™m gi√° tr·ªã l√™n c·ªôt
for bar, (label, count) in zip(bars, binary_counts.items()):
    height = bar.get_height()
    axes[1].text(bar.get_x() + bar.get_width()/2., height,
                f'{count:,}\n({count/len(df)*100:.1f}%)',
                ha='center', va='bottom', fontsize=12, fontweight='bold')

plt.tight_layout()
plt.savefig(os.path.join(BACKUP_FOLDER, 'label_distribution.png'), dpi=300, bbox_inches='tight')
print(f"‚úì ƒê√£ l∆∞u bi·ªÉu ƒë·ªì: {BACKUP_FOLDER}/label_distribution.png")
plt.close()  # ‚Üê FIX: ƒê√≥ng figure thay v√¨ show() (tr√°nh crash tr√™n WSL)

# ================================================================================
# 3. DATA PREPROCESSING
# ================================================================================

print("\n" + "=" * 100)
print("STEP 3: TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU")
print("=" * 100)

# T√°ch features v√† labels
X = df.drop([label_col, 'binary_label'], axis=1)
y = df['binary_label']

print(f"\nShape ban ƒë·∫ßu:")
print(f"  X: {X.shape}")
print(f"  y: {y.shape}")

# X·ª≠ l√Ω missing values
print(f"\n‚Üí Missing values: {X.isnull().sum().sum()}")
if X.isnull().sum().sum() > 0:
    print("  Filling missing values with 0...")
    X = X.fillna(0)

# X·ª≠ l√Ω infinite values
print(f"‚Üí Infinite values: {np.isinf(X.values).sum()}")
if np.isinf(X.values).sum() > 0:
    print("  Replacing infinite values with 0...")
    X = X.replace([np.inf, -np.inf], 0)

# Chuy·ªÉn t·∫•t c·∫£ v·ªÅ numeric
print("‚Üí Converting all columns to numeric...")
for col in X.columns:
    X[col] = pd.to_numeric(X[col], errors='coerce')
X = X.fillna(0)

# Lo·∫°i b·ªè constant columns
constant_cols = [col for col in X.columns if X[col].nunique() <= 1]
if constant_cols:
    print(f"‚Üí Lo·∫°i b·ªè {len(constant_cols)} constant columns")
    X = X.drop(constant_cols, axis=1)

print(f"\nShape sau x·ª≠ l√Ω:")
print(f"  X: {X.shape}")

# Encode labels
le = LabelEncoder()
y_encoded = le.fit_transform(y)
label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))

print(f"\nLabel mapping: {label_mapping}")
print(f"  {le.classes_[0]} = {label_mapping[le.classes_[0]]}")
print(f"  {le.classes_[1]} = {label_mapping[le.classes_[1]]}")

# ================================================================================
# 4. SPLIT DATA
# ================================================================================

print("\n" + "=" * 100)
print("STEP 4: CHIA D·ªÆ LI·ªÜU")
print("=" * 100)

TEST_SIZE = 0.2
VAL_SIZE = 0.125  # 10% of total = 0.125 of train_val

# Split train+val and test
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X, y_encoded, test_size=TEST_SIZE, random_state=42, stratify=y_encoded
)

# Split train and validation
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=VAL_SIZE, random_state=42, stratify=y_train_val
)

print(f"\nT·ªïng s·ªë m·∫´u: {len(X):,}")
print(f"  Training:   {len(X_train):>8,} ({len(X_train)/len(X)*100:>5.1f}%)")
print(f"  Validation: {len(X_val):>8,} ({len(X_val)/len(X)*100:>5.1f}%)")
print(f"  Test:       {len(X_test):>8,} ({len(X_test)/len(X)*100:>5.1f}%)")

# Ki·ªÉm tra ph√¢n b·ªë labels trong m·ªói t·∫≠p
print("\nPh√¢n b·ªë labels trong t·ª´ng t·∫≠p:")
print("-" * 100)
# ‚ö†Ô∏è FIX: D√πng th·ª© t·ª± ƒë√∫ng theo LabelEncoder (Attack=0, Benign=1)
print(f"{'T·∫≠p':<15s} {le.classes_[0]:>12s} {le.classes_[1]:>12s}")
print("-" * 100)

for name, y_set in [('Training', y_train), ('Validation', y_val), ('Test', y_test)]:
    unique, counts = np.unique(y_set, return_counts=True)
    # Class 0 l√† Attack, Class 1 l√† Benign (theo alphabet)
    class0_count = counts[0] if unique[0] == 0 else counts[1]
    class1_count = counts[1] if unique[0] == 0 else counts[0]
    print(f"{name:<15s} {class0_count:>12,} {class1_count:>12,}")

# ================================================================================
# 5. DATA NORMALIZATION
# ================================================================================

print("\n" + "=" * 100)
print("STEP 5: CHU·∫®N H√ìA D·ªÆ LI·ªÜU")
print("=" * 100)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

print("\n‚úì ƒê√£ chu·∫©n h√≥a d·ªØ li·ªáu b·∫±ng StandardScaler")
print(f"  Mean: {scaler.mean_[:5]}... (first 5 features)")
print(f"  Std:  {scaler.scale_[:5]}... (first 5 features)")

# ================================================================================
# 6. BUILD CNN-GRU MODEL
# ================================================================================

print("\n" + "=" * 100)
print("STEP 6: X√ÇY D·ª∞NG M√î H√åNH CNN-GRU")
print("=" * 100)

def build_cnn_gru_model(input_shape, num_classes=2):
    """
    X√¢y d·ª±ng m√¥ h√¨nh CNN-GRU theo ki·∫øn tr√∫c DeepFed:
    - CNN Module: 3 Conv blocks
    - GRU Module: 2 GRU layers  
    - MLP Module: 2 Dense layers
    - Softmax output
    """
    
    input_layer = layers.Input(shape=input_shape, name='input')
    
    # ===== CNN MODULE =====
    print("\n‚Üí Building CNN Module...")
    x_cnn = layers.Reshape((input_shape[0], 1), name='reshape_cnn')(input_layer)
    
    # Conv Block 1
    x_cnn = layers.Conv1D(filters=64, kernel_size=3, padding='same', 
                          activation='relu', name='conv1')(x_cnn)
    x_cnn = layers.BatchNormalization(name='bn1')(x_cnn)
    x_cnn = layers.MaxPooling1D(pool_size=2, name='pool1')(x_cnn)
    
    # Conv Block 2
    x_cnn = layers.Conv1D(filters=128, kernel_size=3, padding='same',
                          activation='relu', name='conv2')(x_cnn)
    x_cnn = layers.BatchNormalization(name='bn2')(x_cnn)
    x_cnn = layers.MaxPooling1D(pool_size=2, name='pool2')(x_cnn)
    
    # Conv Block 3
    x_cnn = layers.Conv1D(filters=256, kernel_size=3, padding='same',
                          activation='relu', name='conv3')(x_cnn)
    x_cnn = layers.BatchNormalization(name='bn3')(x_cnn)
    x_cnn = layers.MaxPooling1D(pool_size=2, name='pool3')(x_cnn)
    
    # Flatten
    cnn_output = layers.Flatten(name='flatten_cnn')(x_cnn)
    
    # ===== GRU MODULE =====
    print("‚Üí Building GRU Module...")
    x_gru = layers.Reshape((input_shape[0], 1), name='reshape_gru')(input_layer)
    
    # GRU Layer 1
    x_gru = layers.GRU(units=128, return_sequences=True, name='gru1')(x_gru)
    
    # GRU Layer 2
    x_gru = layers.GRU(units=64, return_sequences=False, name='gru2')(x_gru)
    
    gru_output = x_gru
    
    # ===== CONCATENATE =====
    print("‚Üí Concatenating CNN and GRU outputs...")
    concatenated = layers.Concatenate(name='concatenate')([cnn_output, gru_output])
    
    # ===== MLP MODULE =====
    print("‚Üí Building MLP Module...")
    # Dense Layer 1
    x = layers.Dense(256, activation='relu', name='dense1')(concatenated)
    x = layers.BatchNormalization(name='bn_mlp1')(x)
    x = layers.Dropout(0.5, name='dropout1')(x)
    
    # Dense Layer 2
    x = layers.Dense(128, activation='relu', name='dense2')(x)
    x = layers.BatchNormalization(name='bn_mlp2')(x)
    x = layers.Dropout(0.3, name='dropout2')(x)
    
    # ===== OUTPUT =====
    output = layers.Dense(num_classes, activation='softmax', name='output')(x)
    
    # Create model
    model = models.Model(inputs=input_layer, outputs=output, name='CNN_GRU_Model')
    
    return model

# Build model
input_shape = (X_train_scaled.shape[1],)
model = build_cnn_gru_model(input_shape, num_classes=2)

# Compile
print("\n‚Üí Compiling model...")
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

print("\n‚úì ƒê√£ x√¢y d·ª±ng v√† compile model")
print(f"  T·ªïng s·ªë parameters: {model.count_params():,}")

# Model summary
print("\n" + "=" * 100)
print("KI·∫æN TR√öC M√î H√åNH")
print("=" * 100)
model.summary()

# ================================================================================
# 7. TRAIN MODEL
# ================================================================================

print("\n" + "=" * 100)
print("STEP 7: HU·∫§N LUY·ªÜN M√î H√åNH")
print("=" * 100)

# V·ªõi 20 files (~14-15M samples), 20 epochs l√† ƒë·ªß ƒë·ªÉ model h·ªôi t·ª•
EPOCHS = 20  # Gi·∫£m xu·ªëng 20 epochs (ƒë·ªß cho dataset l·ªõn)
BATCH_SIZE = 2048  # TƒÉng batch size t·ªëi ƒëa ƒë·ªÉ t·∫≠n d·ª•ng GPU 8GB

print(f"\nHyperparameters:")
print(f"  Epochs: {EPOCHS} (20 epochs ƒë·ªß cho 20 files data)")
print(f"  Batch size: {BATCH_SIZE}")
print(f"  Optimizer: Adam (lr=0.001)")
print(f"  Loss: Sparse Categorical Crossentropy")

# Callbacks
callbacks = [
    EarlyStopping(
        monitor='val_loss',
        patience=10,  # Paper DeepFed: patience 10-15 epochs
        restore_best_weights=True,
        verbose=1
    ),
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,  # Gi·∫£m learning rate sau 5 epochs kh√¥ng c·∫£i thi·ªán
        min_lr=1e-7,
        verbose=1
    ),
    ModelCheckpoint(
        os.path.join(BACKUP_FOLDER, 'best_model.h5'),
        monitor='val_accuracy',
        save_best_only=True,
        verbose=1
    )
    # Note: Checkpoint t·ª± ƒë·ªông s·∫Ω l∆∞u best model, kh√¥ng c·∫ßn l∆∞u m·ªói 5 epochs n·ªØa
]

# ================================================================================
# CALCULATE CLASS WEIGHTS (ƒë·ªÉ x·ª≠ l√Ω imbalanced data)
# ================================================================================

from sklearn.utils.class_weight import compute_class_weight

print("\n" + "-" * 100)
print("T√çNH TO√ÅN CLASS WEIGHTS")
print("-" * 100)

# T√≠nh class weights t·ª± ƒë·ªông
class_weights_array = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weights = dict(enumerate(class_weights_array))

print(f"\n‚Üí Class Weights (ƒë·ªÉ x·ª≠ l√Ω imbalanced data - ratio {imbalance_ratio:.2f}:1):")
print(f"   Class 0 ({le.classes_[0]}): {class_weights[0]:.6f}")
print(f"   Class 1 ({le.classes_[1]}): {class_weights[1]:.6f}")
print(f"\n‚Üí Benign class ƒë∆∞·ª£c tƒÉng tr·ªçng s·ªë {class_weights[1]/class_weights[0]:.2f}x so v·ªõi Attack")
print(f"   ‚Üí Model s·∫Ω ch√∫ √Ω nhi·ªÅu h∆°n v√†o minority class (Benign)")
print(f"   ‚Üí C·∫£i thi·ªán Recall v√† F1-score cho class Benign")

print("\nB·∫Øt ƒë·∫ßu training...\n")
print("‚è∞ Th·ªùi gian d·ª± ki·∫øn: ~2-3 gi·ªù cho 20 epochs v·ªõi 20 files")
print("üíæ Model s·∫Ω t·ª± ƒë·ªông l∆∞u:")
print(f"   - {BACKUP_FOLDER}/best_model.h5: L∆∞u model t·ªët nh·∫•t")
print("\n" + "=" * 100 + "\n")

# Training with error handling
try:
    history = model.fit(
        X_train_scaled, y_train,
        validation_data=(X_val_scaled, y_val),
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        callbacks=callbacks,
        class_weight=class_weights,  # ‚Üê TH√äM CLASS WEIGHTS ƒë·ªÉ x·ª≠ l√Ω imbalanced data
        verbose=1
    )
    print("\n‚úì Ho√†n th√†nh training!")
    
except KeyboardInterrupt:
    print("\n\n‚ö†Ô∏è  TRAINING B·ªä D·ª™NG B·ªûI NG∆Ø·ªúI D√ôNG (Ctrl+C)")
    print(f"‚Üí Model t·ªët nh·∫•t ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {BACKUP_FOLDER}/best_model.h5")
    print("‚Üí Checkpoint cu·ªëi c√πng c√≥ th·ªÉ xem trong folder")
    raise
    
except Exception as e:
    print(f"\n\n‚ùå L·ªñI TRONG QU√Å TR√åNH TRAINING: {e}")
    print("‚Üí Ki·ªÉm tra l·∫°i GPU memory ho·∫∑c RAM")
    print(f"‚Üí Model t·ªët nh·∫•t tr∆∞·ªõc khi l·ªói ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {BACKUP_FOLDER}/best_model.h5")
    raise

print("\n‚úì Ho√†n th√†nh training!")

# ================================================================================
# 8. PLOT TRAINING HISTORY
# ================================================================================

print("\n" + "=" * 100)
print("STEP 8: VISUALIZE QU√Å TR√åNH TRAINING")
print("=" * 100)

fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Loss
axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2, marker='o')
axes[0].plot(history.history['val_loss'], label='Val Loss', linewidth=2, marker='s')
axes[0].set_title('Model Loss', fontsize=16, fontweight='bold', pad=15)
axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')
axes[0].set_ylabel('Loss', fontsize=12, fontweight='bold')
axes[0].legend(fontsize=11)
axes[0].grid(True, alpha=0.3)

# Accuracy
axes[1].plot(history.history['accuracy'], label='Train Acc', linewidth=2, marker='o')
axes[1].plot(history.history['val_accuracy'], label='Val Acc', linewidth=2, marker='s')
axes[1].set_title('Model Accuracy', fontsize=16, fontweight='bold', pad=15)
axes[1].set_xlabel('Epoch', fontsize=12, fontweight='bold')
axes[1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')
axes[1].legend(fontsize=11)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(BACKUP_FOLDER, 'training_history.png'), dpi=300, bbox_inches='tight')
print(f"\n‚úì ƒê√£ l∆∞u bi·ªÉu ƒë·ªì training history: {BACKUP_FOLDER}/training_history.png")
plt.close()  # ‚Üê FIX: ƒê√≥ng figure thay v√¨ show()

# ================================================================================
# 9. EVALUATE MODEL
# ================================================================================

print("\n" + "=" * 100)
print("STEP 9: ƒê√ÅNH GI√Å M√î H√åNH TR√äN T·∫¨P TEST")
print("=" * 100)

# Predictions
print("\n‚Üí ƒêang th·ª±c hi·ªán predictions tr√™n t·∫≠p test...")
y_pred_proba = model.predict(X_test_scaled, verbose=0)
y_pred = np.argmax(y_pred_proba, axis=1)

# Calculate metrics
accuracy = accuracy_score(y_test, y_pred)

# ‚ö†Ô∏è QUAN TR·ªåNG: pos_label=0 v√¨ Attack (class 0) l√† minority class quan tr·ªçng c·∫ßn detect
# Class 0 = Attack (2.3%), Class 1 = Benign (97.7%)
# Trong Intrusion Detection, Attack l√† "positive" class (c·∫ßn ph√°t hi·ªán)
precision = precision_score(y_test, y_pred, average='binary', pos_label=0)
recall = recall_score(y_test, y_pred, average='binary', pos_label=0)
f1 = f1_score(y_test, y_pred, average='binary', pos_label=0)

print("\n" + "=" * 100)
print("K·∫æT QU·∫¢ ƒê√ÅNH GI√Å")
print("=" * 100)
print(f"\n‚ö†Ô∏è  L∆∞u √Ω: Metrics d∆∞·ªõi ƒë√¢y t√≠nh cho class {le.classes_[0]} (pos_label=0)")
print(f"  ‚Üí {le.classes_[0]} l√† minority class quan tr·ªçng c·∫ßn ph√°t hi·ªán\n")
print(f"  ACCURACY:  {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"  PRECISION: {precision:.4f} ({precision*100:.2f}%)  ‚Üê Precision c·ªßa {le.classes_[0]}")
print(f"  RECALL:    {recall:.4f} ({recall*100:.2f}%)  ‚Üê Recall c·ªßa {le.classes_[0]}")
print(f"  F1-SCORE:  {f1:.4f} ({f1*100:.2f}%)  ‚Üê F1-Score c·ªßa {le.classes_[0]}")

# Classification Report
print("\n" + "-" * 100)
print("CLASSIFICATION REPORT CHI TI·∫æT")
print("-" * 100)

# ‚ö†Ô∏è QUAN TR·ªåNG: LabelEncoder t·ª± ƒë·ªông sort theo alphabet
# Attack (A) ‚Üí 0, Benign (B) ‚Üí 1
# N√™n target_names ph·∫£i theo ƒë√∫ng th·ª© t·ª± n√†y!
print(f"\n‚ö†Ô∏è  L∆∞u √Ω: Label encoding:")
print(f"   Class 0 = {le.classes_[0]}")
print(f"   Class 1 = {le.classes_[1]}\n")

report = classification_report(y_test, y_pred, 
                               target_names=[le.classes_[0], le.classes_[1]],  # ‚Üê FIX: D√πng th·ª© t·ª± ƒë√∫ng
                               digits=4)
print(report)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()

print("\n" + "-" * 100)
print("CONFUSION MATRIX")
print("-" * 100)
print(f"\n‚ö†Ô∏è  L∆∞u √Ω: Class 0 = {le.classes_[0]}, Class 1 = {le.classes_[1]}")
print(f"\n  True Negatives  (TN): {tn:>10,}  ‚Üê {le.classes_[0]} d·ª± ƒëo√°n ƒë√∫ng l√† {le.classes_[0]}")
print(f"  False Positives (FP): {fp:>10,}  ‚Üê {le.classes_[0]} nh·∫ßm th√†nh {le.classes_[1]}")
print(f"  False Negatives (FN): {fn:>10,}  ‚Üê {le.classes_[1]} nh·∫ßm th√†nh {le.classes_[0]}")
print(f"  True Positives  (TP): {tp:>10,}  ‚Üê {le.classes_[1]} d·ª± ƒëo√°n ƒë√∫ng l√† {le.classes_[1]}")

# Plot Confusion Matrix
plt.figure(figsize=(10, 8))

# ‚ö†Ô∏è FIX: D√πng th·ª© t·ª± label ƒë√∫ng theo LabelEncoder
# Attack (class 0) tr∆∞·ªõc, Benign (class 1) sau
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
           xticklabels=[le.classes_[0], le.classes_[1]],  # ‚Üê FIX: Attack, Benign
           yticklabels=[le.classes_[0], le.classes_[1]],  # ‚Üê FIX: Attack, Benign
           cbar_kws={'label': 'Count'},
           annot_kws={'fontsize': 14, 'fontweight': 'bold'})
plt.title('Confusion Matrix', fontsize=18, fontweight='bold', pad=20)
plt.ylabel('True Label', fontsize=14, fontweight='bold')
plt.xlabel('Predicted Label', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig(os.path.join(BACKUP_FOLDER, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')
print(f"\n‚úì ƒê√£ l∆∞u confusion matrix: {BACKUP_FOLDER}/confusion_matrix.png")
plt.close()  # ‚Üê FIX: ƒê√≥ng figure thay v√¨ show()

# ================================================================================
# 10. SAVE RESULTS
# ================================================================================

print("\n" + "=" * 100)
print("STEP 10: L∆ØU K·∫æT QU·∫¢")
print("=" * 100)

# Save model
model.save(os.path.join(BACKUP_FOLDER, 'final_cnn_gru_model.h5'))
print(f"\n‚úì ƒê√£ l∆∞u model: {BACKUP_FOLDER}/final_cnn_gru_model.h5")

# Save scaler and label encoder
joblib.dump(scaler, os.path.join(BACKUP_FOLDER, 'scaler.pkl'))
print(f"‚úì ƒê√£ l∆∞u scaler: {BACKUP_FOLDER}/scaler.pkl")

joblib.dump(le, os.path.join(BACKUP_FOLDER, 'label_encoder.pkl'))
print(f"‚úì ƒê√£ l∆∞u label encoder: {BACKUP_FOLDER}/label_encoder.pkl")

# Save detailed results
with open(os.path.join(BACKUP_FOLDER, 'results_summary.txt'), 'w', encoding='utf-8') as f:
    f.write("=" * 100 + "\n")
    f.write(" " * 30 + "K·∫æT QU·∫¢ ƒê√ÅNH GI√Å M√î H√åNH CNN-GRU\n")
    f.write("=" * 100 + "\n\n")
    
    f.write("DATASET INFORMATION\n")
    f.write("-" * 100 + "\n")
    f.write(f"Dataset: IoT Dataset 2023\n")
    f.write(f"Total samples: {len(df):,}\n")
    f.write(f"Number of features: {X_train_scaled.shape[1]}\n")
    f.write(f"Number of classes: 2 (Benign, Attack)\n\n")
    
    f.write("DATA SPLIT\n")
    f.write("-" * 100 + "\n")
    f.write(f"Training:   {len(X_train):>10,} ({len(X_train)/len(X)*100:>5.1f}%)\n")
    f.write(f"Validation: {len(X_val):>10,} ({len(X_val)/len(X)*100:>5.1f}%)\n")
    f.write(f"Test:       {len(X_test):>10,} ({len(X_test)/len(X)*100:>5.1f}%)\n\n")
    
    f.write("MODEL ARCHITECTURE\n")
    f.write("-" * 100 + "\n")
    f.write(f"Model: CNN-GRU (DeepFed Architecture)\n")
    f.write(f"Total parameters: {model.count_params():,}\n\n")
    
    f.write("TRAINING CONFIGURATION\n")
    f.write("-" * 100 + "\n")
    f.write(f"Epochs: {EPOCHS}\n")
    f.write(f"Batch size: {BATCH_SIZE}\n")
    f.write(f"Optimizer: Adam (lr=0.001)\n")
    f.write(f"Loss function: Sparse Categorical Crossentropy\n\n")
    
    f.write("PERFORMANCE METRICS\n")
    f.write("-" * 100 + "\n")
    f.write(f"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\n")
    f.write(f"Precision: {precision:.4f} ({precision*100:.2f}%)\n")
    f.write(f"Recall:    {recall:.4f} ({recall*100:.2f}%)\n")
    f.write(f"F1-Score:  {f1:.4f} ({f1*100:.2f}%)\n\n")
    
    f.write("CONFUSION MATRIX\n")
    f.write("-" * 100 + "\n")
    f.write(f"True Negatives  (TN): {tn:,}\n")
    f.write(f"False Positives (FP): {fp:,}\n")
    f.write(f"False Negatives (FN): {fn:,}\n")
    f.write(f"True Positives  (TP): {tp:,}\n\n")
    
    f.write("CLASSIFICATION REPORT\n")
    f.write("-" * 100 + "\n")
    f.write(report)

print(f"‚úì ƒê√£ l∆∞u k·∫øt qu·∫£ chi ti·∫øt: {BACKUP_FOLDER}/results_summary.txt")

# Save training config info
with open(os.path.join(BACKUP_FOLDER, 'training_config.txt'), 'w', encoding='utf-8') as f:
    f.write("=" * 100 + "\n")
    f.write("TRAINING CONFIGURATION\n")
    f.write("=" * 100 + "\n\n")
    f.write(f"Timestamp: {TIMESTAMP}\n")
    f.write(f"Backup Folder: {BACKUP_FOLDER}\n\n")
    f.write(f"Dataset Files: 20 files\n")
    f.write(f"Total Samples: {len(df):,}\n")
    f.write(f"Features: {X_train_scaled.shape[1]}\n\n")
    f.write(f"Epochs: {EPOCHS}\n")
    f.write(f"Batch Size: {BATCH_SIZE}\n")
    f.write(f"Learning Rate: 0.001\n")
    f.write(f"Optimizer: Adam\n")
    f.write(f"Class Weights: Yes (balanced)\n")
    f.write(f"  - Class 0 ({le.classes_[0]}): {class_weights[0]:.6f}\n")
    f.write(f"  - Class 1 ({le.classes_[1]}): {class_weights[1]:.6f}\n")
    f.write(f"  - Weight ratio: {class_weights[1]/class_weights[0]:.2f}x\n")

print(f"‚úì ƒê√£ l∆∞u c·∫•u h√¨nh training: {BACKUP_FOLDER}/training_config.txt")

# ================================================================================
# FINAL SUMMARY
# ================================================================================

print("\n" + "=" * 100)
print(" " * 40 + "HO√ÄN TH√ÄNH!")
print("=" * 100)

print("\nüìä K·∫æT QU·∫¢ CU·ªêI C√ôNG:")
print("-" * 100)
print(f"  ‚úì Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"  ‚úì Precision: {precision:.4f} ({precision*100:.2f}%)")
print(f"  ‚úì Recall:    {recall:.4f} ({recall*100:.2f}%)")
print(f"  ‚úì F1-Score:  {f1:.4f} ({f1*100:.2f}%)")

print("\nüìÅ C√ÅC FILE ƒê√É L∆ØU:")
print("-" * 100)
print(f"  üìÇ Th∆∞ m·ª•c backup: {BACKUP_FOLDER}/")
print(f"  ‚úì final_cnn_gru_model.h5      - Model ƒë√£ train")
print(f"  ‚úì best_model.h5               - Model t·ªët nh·∫•t (t·ª´ checkpoint)")
print(f"  ‚úì scaler.pkl                  - StandardScaler")
print(f"  ‚úì label_encoder.pkl           - LabelEncoder")
print(f"  ‚úì results_summary.txt         - K·∫øt qu·∫£ chi ti·∫øt")
print(f"  ‚úì training_config.txt         - C·∫•u h√¨nh training")
print(f"  ‚úì label_distribution.png      - Bi·ªÉu ƒë·ªì ph√¢n b·ªë nh√£n")
print(f"  ‚úì training_history.png        - Qu√° tr√¨nh training")
print(f"  ‚úì confusion_matrix.png        - Confusion matrix")

print("\n" + "=" * 100)
print(" " * 35 + "C·∫£m ∆°n b·∫°n ƒë√£ s·ª≠ d·ª•ng!")
print("=" * 100 + "\n")